# WS-ADR-041-001: Prompt Template Include System Implementation

**Status:** Accepted  
**Created:** 2026-01-22  
**ADR Reference:** ADR-041 (Accepted)  
**Scope:** Multi-commit  

---

## 1. Objective

Implement the Prompt Template Include System as defined in ADR-041. This system enables prompt assembly from generic templates plus workflow-driven includes, with deterministic output, SHA-256 hashing, and full audit logging.

**Core Deliverable:** `PromptAssembler` class that produces byte-identical, hashable, replay-stable prompts.

---

## 2. Success Criteria

1. PromptAssembler produces deterministic output (same inputs = byte-identical output)
2. Workflow Tokens (`$$SECTION_NAME`) resolve from workflow `includes` map
3. Template Includes (`$$include <path>`) resolve from file paths
4. All failure modes produce explicit, typed errors
5. SHA-256 hash computed and logged per ADR-010
6. CRLF normalized to LF (canonical encoding)
7. All golden tests pass
8. CI prompt compile job catches drift at build time
9. At least one prompt (PGC) converted to template pattern

---

## 3. Scope

### In Scope

- `PromptAssembler` class implementation
- Token resolution (Workflow Tokens + Template Includes)
- Canonical encoding (UTF-8, LF normalization)
- Error types (`UnresolvedTokenError`, `IncludeNotFoundError`, `NestedTokenError`, `EncodingError`)
- Audit logging integration (per ADR-010)
- Test fixtures and golden tests
- CI prompt compile job
- PGC prompt migration (first adopter)

### Out of Scope

- Migrating all existing prompts (future work, per SOP)
- Conditional includes, parameterized includes, recursive includes (prohibited per ADR-041)
- Runtime-generated include content (prohibited per ADR-041)

---

## 4. Prerequisites

- [x] ADR-041 Accepted
- [x] Test plan defined (Section 11b)
- [x] Canonical encoding rules locked (Section 11a)
- [ ] Workflow plan schema exists (`seed/schemas/workflow-plan.v1.json`)

---

## 5. Implementation Phases

### Phase 1: Core Data Structures

**Objective:** Define the domain model for prompt assembly.

**Tasks:**

1.1. Create `app/domain/prompt/assembler.py` with:

```python
@dataclass
class AssembledPrompt:
    """Immutable assembled prompt artifact."""
    content: str
    content_hash: str  # SHA-256
    task_ref: str
    includes_resolved: Dict[str, str]
    assembled_at: datetime
    correlation_id: UUID
```

1.2. Create error types in `app/domain/prompt/errors.py`:

```python
class PromptAssemblyError(Exception):
    """Base class for prompt assembly errors."""
    pass

class UnresolvedTokenError(PromptAssemblyError):
    """Workflow Token has no matching include."""
    def __init__(self, token: str):
        self.token = token
        super().__init__(f"Unresolved token: $${token}")

class IncludeNotFoundError(PromptAssemblyError):
    """Include file does not exist."""
    def __init__(self, path: str):
        self.path = path
        super().__init__(f"Include file not found: {path}")

class NestedTokenError(PromptAssemblyError):
    """Include file contains tokens (prohibited)."""
    def __init__(self, path: str, token: str):
        self.path = path
        self.token = token
        super().__init__(f"Nested token $${token} found in {path}")

class EncodingError(PromptAssemblyError):
    """File is not valid UTF-8."""
    def __init__(self, path: str):
        self.path = path
        super().__init__(f"Invalid UTF-8 encoding: {path}")
```

**Acceptance Criteria:**
- Data classes defined with all required fields
- Error types have typed attributes for assertion in tests
- No external dependencies in this phase

**Prohibited:**
- Do not add any assembly logic yet
- Do not import from other modules that don't exist

---

### Phase 2: Token Scanner

**Objective:** Implement token detection with regex patterns.

**Tasks:**

2.1. Add token patterns to `app/domain/prompt/assembler.py`:

```python
import re

# Workflow Token: $$SECTION_NAME (must be on own line)
WORKFLOW_TOKEN_PATTERN = re.compile(r'^(\$\$([A-Z][A-Z0-9_]*))\s*$', re.MULTILINE)

# Template Include: $$include <path> (must be on own line)
TEMPLATE_INCLUDE_PATTERN = re.compile(r'^(\$\$include\s+(.+))\s*$', re.MULTILINE)
```

2.2. Implement `_scan_workflow_tokens(content: str) -> List[Tuple[str, str]]`:
- Returns list of (full_match, token_name) tuples
- Preserves lexical order

2.3. Implement `_scan_template_includes(content: str) -> List[Tuple[str, str]]`:
- Returns list of (full_match, path) tuples
- Preserves lexical order

2.4. Implement `_has_tokens(content: str) -> bool`:
- Returns True if content contains any token patterns
- Used to detect nested tokens in includes

**Acceptance Criteria:**
- Patterns match only tokens on their own line
- Lexical order preserved
- No false positives on inline `$$` text

**Prohibited:**
- Do not strip or normalize whitespace yet
- Do not resolve tokens yet

---

### Phase 3: File Loading with Canonical Encoding

**Objective:** Load files with UTF-8 + LF normalization.

**Tasks:**

3.1. Implement `_load_file(path: str) -> str`:

```python
def _load_file(self, path: str) -> str:
    """Load file with canonical encoding.
    
    - UTF-8 decoding (raises EncodingError if invalid)
    - CRLF -> LF normalization
    - No BOM handling needed (UTF-8 without BOM per ADR-041)
    """
    full_path = self._resolve_path(path)
    
    if not full_path.exists():
        raise IncludeNotFoundError(path)
    
    try:
        content = full_path.read_bytes().decode('utf-8')
    except UnicodeDecodeError:
        raise EncodingError(path)
    
    # Canonical newline normalization
    content = content.replace('\r\n', '\n').replace('\r', '\n')
    
    return content
```

3.2. Implement `_resolve_path(path: str) -> Path`:
- Resolve relative to repository root
- Validate path is within allowed roots (seed/, tests/fixtures/)

3.3. Implement `_load_template(task_ref: str) -> str`:
- Load from `seed/prompts/tasks/{task_ref}.txt`
- Apply canonical encoding

**Acceptance Criteria:**
- Invalid UTF-8 raises `EncodingError`
- Missing file raises `IncludeNotFoundError`
- CRLF normalized to LF
- Paths resolved relative to repo root

**Prohibited:**
- Do not allow paths outside allowed roots
- Do not cache file contents (stateless per ADR-040)

---

### Phase 4: Token Resolution

**Objective:** Replace tokens with include content.

**Tasks:**

4.1. Implement `_resolve_workflow_tokens(content: str, includes: Dict[str, str]) -> str`:

```python
def _resolve_workflow_tokens(self, content: str, includes: Dict[str, str]) -> str:
    """Resolve $$SECTION_NAME tokens from workflow includes map.
    
    Process in lexical order. Fail on first unresolved token.
    """
    tokens = self._scan_workflow_tokens(content)
    
    for full_match, token_name in tokens:
        if token_name not in includes:
            raise UnresolvedTokenError(token_name)
        
        include_path = includes[token_name]
        include_content = self._load_file(include_path)
        
        # Check for nested tokens (prohibited)
        if self._has_tokens(include_content):
            nested = self._scan_workflow_tokens(include_content) + self._scan_template_includes(include_content)
            raise NestedTokenError(include_path, nested[0][1])
        
        content = content.replace(full_match, include_content, 1)
    
    return content
```

4.2. Implement `_resolve_template_includes(content: str) -> str`:

```python
def _resolve_template_includes(self, content: str) -> str:
    """Resolve $$include <path> tokens from file system.
    
    Process in lexical order. Fail on first missing file.
    """
    includes = self._scan_template_includes(content)
    
    for full_match, path in includes:
        include_content = self._load_file(path.strip())
        
        # Check for nested tokens (prohibited)
        if self._has_tokens(include_content):
            nested = self._scan_workflow_tokens(include_content) + self._scan_template_includes(include_content)
            raise NestedTokenError(path, nested[0][1])
        
        content = content.replace(full_match, include_content, 1)
    
    return content
```

4.3. Implement `_validate_no_unresolved_tokens(content: str) -> None`:
- Raise if any tokens remain after resolution
- Safety check for typos or missing includes

**Acceptance Criteria:**
- Tokens resolved in lexical order
- Nested tokens detected and rejected
- Unresolved tokens cause explicit failure

**Prohibited:**
- Do not reorder tokens
- Do not deduplicate tokens
- Do not allow nested tokens


---

### Phase 5: Assembly and Hashing

**Objective:** Implement the main `assemble()` method with SHA-256 hashing.

**Tasks:**

5.1. Implement `PromptAssembler.assemble()`:

```python
import hashlib
from datetime import datetime
from typing import Dict
from uuid import UUID

class PromptAssembler:
    """Assembles prompts from templates and includes.
    
    Assembly is deterministic: same inputs -> byte-identical output.
    """
    
    def __init__(self, template_root: str = "seed/prompts/tasks"):
        self._template_root = Path(template_root)
    
    def assemble(
        self,
        task_ref: str,
        includes: Dict[str, str],
        correlation_id: UUID
    ) -> AssembledPrompt:
        """
        Load template, resolve tokens in lexical order, return assembled prompt.
        
        Raises:
            UnresolvedTokenError: If any Workflow Token cannot be resolved
            IncludeNotFoundError: If referenced file doesn't exist
            NestedTokenError: If include file contains tokens
            EncodingError: If file is not valid UTF-8
        """
        # 1. Load template
        template = self._load_template(task_ref)
        
        # 2. Resolve Workflow Tokens (from includes map)
        resolved = self._resolve_workflow_tokens(template, includes)
        
        # 3. Resolve Template Includes (from file system)
        resolved = self._resolve_template_includes(resolved)
        
        # 4. Validate no unresolved tokens remain
        self._validate_no_unresolved_tokens(resolved)
        
        # 5. Compute SHA-256 hash on canonical UTF-8 bytes
        content_hash = hashlib.sha256(resolved.encode('utf-8')).hexdigest()
        
        # 6. Return immutable result
        return AssembledPrompt(
            content=resolved,
            content_hash=content_hash,
            task_ref=task_ref,
            includes_resolved=includes.copy(),
            assembled_at=datetime.utcnow(),
            correlation_id=correlation_id
        )
```

**Acceptance Criteria:**
- Same inputs produce byte-identical output
- Hash computed on UTF-8 bytes after all normalization
- `includes_resolved` is a copy (immutable)
- `assembled_at` captured at assembly time

**Prohibited:**
- Do not modify input `includes` dict
- Do not cache results

---

### Phase 6: Test Fixtures

**Objective:** Create test fixtures per ADR-041 Section 11b.

**Tasks:**

6.1. Create directory structure:

```
tests/fixtures/adr041/
  templates/
    clarification_generator_v1.txt
    template_with_missing_token.txt
  includes/
    pgc_context_project_discovery_v1.txt
    clarification_schema_v2.json
    nested_tokens.txt
    non_utf8.bin
    crlf_file.txt
  expected/
    assembled_clarification_generator_project_discovery_v1.txt
    assembled_clarification_generator_project_discovery_v1.sha256
```

6.2. Create `clarification_generator_v1.txt` (minimal template):

```
# Test Template

$$PGC_CONTEXT

---

Schema:
$$OUTPUT_SCHEMA
```

6.3. Create `template_with_missing_token.txt`:

```
# Template with undefined token

$$UNDEFINED_TOKEN
```

6.4. Create `pgc_context_project_discovery_v1.txt`:

```
## Context Block
This is the project discovery context.
```

6.5. Create `clarification_schema_v2.json`:

```json
{"schema": "test"}
```

6.6. Create `nested_tokens.txt`:

```
This include has a $$NESTED token which is prohibited.
```

6.7. Create `non_utf8.bin` with invalid bytes

6.8. Create `crlf_file.txt` with Windows line endings

6.9. Manually assemble expected output and compute SHA-256:

```bash
# Assemble manually, save to expected/
# Compute: sha256sum assembled_clarification_generator_project_discovery_v1.txt
```

**Acceptance Criteria:**
- All fixture files created
- Expected output manually verified
- SHA-256 hash pre-computed and saved

**Prohibited:**
- Do not auto-generate expected output (defeats the purpose)


---

### Phase 7: Golden Tests

**Objective:** Implement all test cases from ADR-041 Section 11b.

**Tasks:**

7.1. Create `tests/tier1/prompt/test_prompt_assembler.py`:

```python
import pytest
from pathlib import Path
from uuid import UUID

from app.domain.prompt.assembler import PromptAssembler, AssembledPrompt
from app.domain.prompt.errors import (
    UnresolvedTokenError,
    IncludeNotFoundError,
    NestedTokenError,
    EncodingError
)

FIXTURES = Path("tests/fixtures/adr041")


class TestPromptAssemblerHappyPath:
    """Golden test: assembled prompt matches expected output byte-for-byte."""
    
    def test_assemble_matches_golden_fixture(self):
        assembler = PromptAssembler(template_root=FIXTURES / "templates")
        
        result = assembler.assemble(
            task_ref="clarification_generator_v1",
            includes={
                "PGC_CONTEXT": str(FIXTURES / "includes/pgc_context_project_discovery_v1.txt"),
                "OUTPUT_SCHEMA": str(FIXTURES / "includes/clarification_schema_v2.json")
            },
            correlation_id=UUID("00000000-0000-0000-0000-000000000001")
        )
        
        expected_content = (FIXTURES / "expected/assembled_clarification_generator_project_discovery_v1.txt").read_text()
        expected_hash = (FIXTURES / "expected/assembled_clarification_generator_project_discovery_v1.sha256").read_text().strip()
        
        assert result.content == expected_content
        assert result.content_hash == expected_hash
        assert result.task_ref == "clarification_generator_v1"
        assert result.includes_resolved == {
            "PGC_CONTEXT": str(FIXTURES / "includes/pgc_context_project_discovery_v1.txt"),
            "OUTPUT_SCHEMA": str(FIXTURES / "includes/clarification_schema_v2.json")
        }
        assert result.correlation_id == UUID("00000000-0000-0000-0000-000000000001")
        assert result.assembled_at is not None


class TestPromptAssemblerFailureModes:
    """All failure modes produce explicit, typed errors."""
    
    def test_unresolved_workflow_token_fails(self):
        assembler = PromptAssembler(template_root=FIXTURES / "templates")
        
        with pytest.raises(UnresolvedTokenError) as exc:
            assembler.assemble(
                task_ref="template_with_missing_token",
                includes={},
                correlation_id=UUID("00000000-0000-0000-0000-000000000002")
            )
        
        assert exc.value.token == "UNDEFINED_TOKEN"
    
    def test_missing_include_file_fails(self):
        assembler = PromptAssembler(template_root=FIXTURES / "templates")
        
        with pytest.raises(IncludeNotFoundError) as exc:
            assembler.assemble(
                task_ref="clarification_generator_v1",
                includes={
                    "PGC_CONTEXT": "tests/fixtures/adr041/includes/does_not_exist.txt",
                    "OUTPUT_SCHEMA": str(FIXTURES / "includes/clarification_schema_v2.json")
                },
                correlation_id=UUID("00000000-0000-0000-0000-000000000003")
            )
        
        assert "does_not_exist.txt" in exc.value.path
    
    def test_nested_tokens_in_include_fails(self):
        assembler = PromptAssembler(template_root=FIXTURES / "templates")
        
        with pytest.raises(NestedTokenError) as exc:
            assembler.assemble(
                task_ref="clarification_generator_v1",
                includes={
                    "PGC_CONTEXT": str(FIXTURES / "includes/nested_tokens.txt"),
                    "OUTPUT_SCHEMA": str(FIXTURES / "includes/clarification_schema_v2.json")
                },
                correlation_id=UUID("00000000-0000-0000-0000-000000000004")
            )
        
        assert "nested_tokens.txt" in exc.value.path
        assert exc.value.token == "NESTED"
    
    def test_non_utf8_include_fails(self):
        assembler = PromptAssembler(template_root=FIXTURES / "templates")
        
        with pytest.raises(EncodingError) as exc:
            assembler.assemble(
                task_ref="clarification_generator_v1",
                includes={
                    "PGC_CONTEXT": str(FIXTURES / "includes/non_utf8.bin"),
                    "OUTPUT_SCHEMA": str(FIXTURES / "includes/clarification_schema_v2.json")
                },
                correlation_id=UUID("00000000-0000-0000-0000-000000000005")
            )
        
        assert "non_utf8.bin" in exc.value.path
    
    def test_crlf_normalized_to_lf(self):
        """CRLF line endings are normalized, hash is stable."""
        assembler = PromptAssembler(template_root=FIXTURES / "templates")
        
        result = assembler.assemble(
            task_ref="clarification_generator_v1",
            includes={
                "PGC_CONTEXT": str(FIXTURES / "includes/crlf_file.txt"),
                "OUTPUT_SCHEMA": str(FIXTURES / "includes/clarification_schema_v2.json")
            },
            correlation_id=UUID("00000000-0000-0000-0000-000000000006")
        )
        
        assert "\r\n" not in result.content
        assert "\r" not in result.content
```

**Acceptance Criteria:**
- All 7 test cases pass
- Error types have correct attributes
- Golden test is byte-exact

**Prohibited:**
- Do not skip any test cases
- Do not use mocks for file loading (test real behavior)


---

### Phase 8: Audit Logging Integration

**Objective:** Integrate with ADR-010 LLM execution logging.

**Tasks:**

8.1. Extend log record schema to include assembly fields:

```python
# In the LLM execution log record
{
    "task_ref": "Clarification Questions Generator v1.0",
    "includes_resolved": {
        "PGC_CONTEXT": "seed/prompts/pgc-contexts/project_discovery.v1.txt",
        "OUTPUT_SCHEMA": "seed/schemas/clarification_question_set.v2.json"
    },
    "assembled_prompt": "# Full assembled prompt...",
    "assembled_prompt_hash": "a1b2c3d4e5f6...",
    "assembly_timestamp": "2026-01-22T15:30:00Z",
    "correlation_id": "uuid-here"
}
```

8.2. Create `log_prompt_assembly()` function or integrate into existing LLM logging:

```python
def log_prompt_assembly(assembled: AssembledPrompt, logger: LLMLogger) -> None:
    """Log assembled prompt per ADR-010."""
    logger.log_assembly(
        task_ref=assembled.task_ref,
        includes_resolved=assembled.includes_resolved,
        assembled_prompt=assembled.content,
        assembled_prompt_hash=assembled.content_hash,
        assembly_timestamp=assembled.assembled_at.isoformat(),
        correlation_id=str(assembled.correlation_id)
    )
```

8.3. Add test for audit log contract:

```python
def test_audit_log_contract(self):
    """Log record contains all required fields per ADR-010."""
    assembler = PromptAssembler(template_root=FIXTURES / "templates")
    mock_logger = MockLLMLogger()
    
    result = assembler.assemble(...)
    log_prompt_assembly(result, mock_logger)
    
    record = mock_logger.last_record
    assert record["task_ref"] == result.task_ref
    assert record["includes_resolved"] == result.includes_resolved
    assert record["assembled_prompt_hash"] == result.content_hash
    assert record["assembled_prompt"] == result.content
    assert record["correlation_id"] == str(result.correlation_id)
    assert "assembly_timestamp" in record
```

**Acceptance Criteria:**
- All ADR-010 required fields logged
- Hash enables replay verification
- Integration test passes

**Prohibited:**
- Do not log to stdout (use structured logger)
- Do not truncate assembled_prompt in log

---

### Phase 9: CI Prompt Compile Job

**Objective:** Catch drift at build time, not production.

**Tasks:**

9.1. Create `ops/scripts/compile_prompts.py`:

```python
#!/usr/bin/env python
"""
Compile all workflow prompts to verify assembly succeeds.

Usage:
    python -m ops.scripts.compile_prompts --output build/prompts/

Exit codes:
    0 - All prompts compiled successfully
    1 - One or more prompts failed to compile
"""

import argparse
import json
import sys
from pathlib import Path
from uuid import uuid4

from app.domain.prompt.assembler import PromptAssembler


def main():
    parser = argparse.ArgumentParser(description="Compile all workflow prompts")
    parser.add_argument("--output", type=Path, default=Path("build/prompts"))
    parser.add_argument("--workflows", type=Path, default=Path("seed/workflows"))
    args = parser.parse_args()
    
    args.output.mkdir(parents=True, exist_ok=True)
    
    assembler = PromptAssembler()
    failures = []
    successes = 0
    
    for workflow_file in args.workflows.glob("*.json"):
        workflow = json.loads(workflow_file.read_text())
        
        for node in workflow.get("nodes", []):
            if "task_ref" not in node:
                continue
                
            node_id = node["node_id"]
            task_ref = node["task_ref"]
            includes = node.get("includes", {})
            
            try:
                result = assembler.assemble(
                    task_ref=task_ref,
                    includes=includes,
                    correlation_id=uuid4()
                )
                
                # Write assembled prompt for inspection
                output_file = args.output / f"{workflow_file.stem}_{node_id}.txt"
                output_file.write_text(result.content)
                
                # Write hash
                hash_file = args.output / f"{workflow_file.stem}_{node_id}.sha256"
                hash_file.write_text(result.content_hash)
                
                print(f"OK  {workflow_file.name}:{node_id} -> {result.content_hash[:12]}...")
                successes += 1
                
            except Exception as e:
                failures.append((workflow_file.name, node_id, str(e)))
                print(f"ERR {workflow_file.name}:{node_id} -> {e}")
    
    print(f"\n{successes} compiled, {len(failures)} failed")
    
    if failures:
        print("\nFailures:")
        for wf, node, err in failures:
            print(f"  - {wf}:{node}: {err}")
        sys.exit(1)
    
    sys.exit(0)


if __name__ == "__main__":
    main()
```

9.2. Create `.github/workflows/prompt-compile.yml`:

```yaml
name: Prompt Compile Check

on:
  push:
    paths:
      - 'seed/prompts/**'
      - 'seed/workflows/**'
      - 'seed/schemas/**'
      - 'app/domain/prompt/**'
  pull_request:
    paths:
      - 'seed/prompts/**'
      - 'seed/workflows/**'
      - 'seed/schemas/**'
      - 'app/domain/prompt/**'

jobs:
  compile-prompts:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Compile all workflow prompts
        run: python -m ops.scripts.compile_prompts --output build/prompts/
      
      - name: Upload compiled prompts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: compiled-prompts
          path: build/prompts/
          retention-days: 7
```

9.3. Add `build/` to `.gitignore` (if not already):

```
# Build artifacts
build/
```

**Acceptance Criteria:**
- Script compiles all workflow prompts
- Non-zero exit on any failure
- CI job triggers on relevant path changes
- Compiled prompts available as artifact

**Prohibited:**
- Do not commit compiled prompts to repo
- Do not skip workflows on failure (report all)


---

### Phase 10: PGC Prompt Migration (First Adopter)

**Objective:** Convert Project Discovery Questions prompt to template pattern.

**Tasks:**

10.1. Create generic template `seed/prompts/tasks/Clarification Questions Generator v1.0.txt`:

```
# Clarification Questions Generator - Canonical Task Prompt v1.0

## Triggering Instruction

You are operating under a **certified role prompt**.
This task prompt defines **only the current task**.

---

$$PGC_CONTEXT

---

## Mode

`questions_only`

## Required Behavior

[Generic rules that apply to ALL PGC prompts - copy from existing]

## Prohibited Language (Strict)

[Generic prohibited language - copy from existing]

## Prohibited Topics (Strict)

[Generic prohibited topics - copy from existing]

## Output Schema Reference

Your output must conform to the following schema:

```json
$$OUTPUT_SCHEMA
```

## Failure Conditions (Automatic Reject)

[Generic failure conditions - copy from existing]
```

10.2. Create context file `seed/prompts/pgc-contexts/project_discovery.v1.txt`:

```
## Replaceable Context Block (Authoritative)

Next Document: Project Discovery Document

Purpose of This Document:
To explore solution space, validate assumptions, identify risks, and define scope 
and constraints - without committing to architecture or implementation.

What This Document Assumes Is Already True:
* The problem statement is correct
* The project intent is valid
* The project type (greenfield, enhancement, etc.) is correct

This Clarification Step Exists To Prevent:
* Starting discovery under the wrong framing
* Discovering too late that the scope category is incorrect
* Producing a discovery document that answers the wrong questions

Questions Asked Here Must:
* Affect how discovery is run
* Change what discovery needs to explore
* Or determine whether discovery should proceed as planned
```

10.3. Update workflow `seed/workflows/pm_discovery.v1.json` to include PGC node with includes:

```json
{
  "node_id": "pgc",
  "type": "pgc",
  "task_ref": "Clarification Questions Generator v1.0",
  "includes": {
    "PGC_CONTEXT": "seed/prompts/pgc-contexts/project_discovery.v1.txt",
    "OUTPUT_SCHEMA": "seed/schemas/clarification_question_set.v2.json"
  }
}
```

10.4. Verify assembly produces valid output:

```bash
python -m ops.scripts.compile_prompts --output build/prompts/
# Should show: pm_discovery.v1.json:pgc -> [hash]
```

10.5. Move old `Project Discovery Questions v1.0.txt` to `recycle/` (deprecated):

```
recycle/prompts/
  Project Discovery Questions v1.0.txt  # Old monolithic prompt
```

**Acceptance Criteria:**
- Generic template created
- Context file created
- Workflow updated with includes
- Prompt compiles successfully
- Output matches expected behavior

**Prohibited:**
- Do not delete old prompt until verified
- Do not change prompt semantics (only structure)

---

## 6. Verification Instructions

After each phase, the human operator should run:

```powershell
# Run tests (operator executes)
python -m pytest tests/tier1/prompt/ -v

# After Phase 9, also run:
python -m ops.scripts.compile_prompts --output build/prompts/
```

**Final verification checklist:**

- [ ] `test_assemble_matches_golden_fixture` passes
- [ ] `test_unresolved_workflow_token_fails` passes
- [ ] `test_missing_include_file_fails` passes
- [ ] `test_nested_tokens_in_include_fails` passes
- [ ] `test_non_utf8_include_fails` passes
- [ ] `test_crlf_normalized_to_lf` passes
- [ ] `test_audit_log_contract` passes
- [ ] `compile_prompts.py` exits 0 for all workflows
- [ ] PGC prompt assembles correctly

---

## 7. Prohibited Actions

- Do NOT run tests automatically (human operator executes)
- Do NOT commit compiled prompts to repo
- Do NOT auto-generate expected golden files
- Do NOT allow nested tokens in includes
- Do NOT allow runtime-generated include content
- Do NOT cache assembly results
- Do NOT modify input includes dict
- Do NOT reorder or deduplicate tokens

---

## 8. Estimated Effort

| Phase | Description | Estimate |
|-------|-------------|----------|
| 1 | Core Data Structures | 1 hour |
| 2 | Token Scanner | 1 hour |
| 3 | File Loading | 1 hour |
| 4 | Token Resolution | 2 hours |
| 5 | Assembly and Hashing | 1 hour |
| 6 | Test Fixtures | 1 hour |
| 7 | Golden Tests | 2 hours |
| 8 | Audit Logging | 1 hour |
| 9 | CI Prompt Compile | 1 hour |
| 10 | PGC Migration | 2 hours |
| **Total** | | **13 hours** |

---

## 9. Files Created

| File | Purpose |
|------|---------|
| `app/domain/prompt/__init__.py` | Package init |
| `app/domain/prompt/assembler.py` | PromptAssembler class |
| `app/domain/prompt/errors.py` | Error types |
| `tests/tier1/prompt/__init__.py` | Test package |
| `tests/tier1/prompt/test_prompt_assembler.py` | Golden tests |
| `tests/fixtures/adr041/templates/*.txt` | Test templates |
| `tests/fixtures/adr041/includes/*` | Test includes |
| `tests/fixtures/adr041/expected/*` | Golden outputs |
| `ops/scripts/compile_prompts.py` | CI compile script |
| `.github/workflows/prompt-compile.yml` | CI workflow |
| `seed/prompts/tasks/Clarification Questions Generator v1.0.txt` | Generic PGC template |
| `seed/prompts/pgc-contexts/project_discovery.v1.txt` | PGC context |

## 10. Files Modified

| File | Change |
|------|--------|
| `seed/workflows/pm_discovery.v1.json` | Add PGC node with includes |
| `.gitignore` | Add `build/` |

---

## 11. References

- ADR-041: Prompt Template Include System (Accepted)
- ADR-010: LLM Execution Logging
- ADR-012: Interaction Model (PGC Amendment)
- ADR-040: Stateless LLM Execution Invariant
- POL-WS-001: Work Statement Standard

---

## 12. Approval

**Work Statement Status:** Accepted

- [x] Technical review complete
- [x] Scope confirmed as multi-commit
- [x] Human operator accepts

---

_Created: 2026-01-22_
