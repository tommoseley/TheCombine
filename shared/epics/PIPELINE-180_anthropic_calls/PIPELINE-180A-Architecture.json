```json
{
  "epic_analysis": "This Epic requires a clean separation between prompt management (DB-backed), API execution (Anthropic client), and envelope structure (standardized JSON responses). Key ambiguities resolved: (1) Prompt rendering will use simple placeholder substitution initially, with Jinja2 as future enhancement; (2) Parse failures return HTTP 200 with parse_error populated, not HTTP 500; (3) Token limits enforced at API layer via max_tokens config, not pre-validation; (4) Prompt versioning uses is_active flag for now, multi-version support deferred; (5) Input format distinction (plain_text vs json) is metadata only - both stored as strings in payload field. Major architectural risk: Anthropic API failures could cascade without retry logic, mitigated by clear error envelopes and client-level timeout configuration. Secondary risk: Large prompts (>100KB) could cause performance issues, mitigated by input size validation at router level. The three-worker split maps cleanly to our architectural concerns: API boundaries (Worker A), persistence contracts (Worker B), and external integration patterns (Worker C).",

  "system_architecture": "The system follows a layered architecture: API Layer (FastAPI router) → Service Layer (QueryEngineService orchestrator) → Repository Layer (PromptRepository) + Client Layer (AnthropicClientAdapter) → External Systems (PostgreSQL DB + Anthropic API). Request flow: HTTP POST with role/payload → Router validates request → Service loads prompt template from DB → Service renders prompt with payload → Service calls Anthropic via adapter → Service attempts JSON parse → Service constructs envelope → Router returns JSON response. All components communicate via typed Python objects (Pydantic models for API, domain models for internal). No caching in v1 to maintain simplicity. Error handling strategy: Repository errors (missing prompt) → HTTP 400; Anthropic errors (rate limit, timeout) → HTTP 200 with error envelope; Parse errors (invalid JSON from Claude) → HTTP 200 with parse_error populated. This ensures calling code always receives parseable JSON. Configuration managed via environment variables (ANTHROPIC_API_KEY, ANTHROPIC_MODEL_NAME, max tokens, timeout). Database schema supports future multi-version prompts but v1 uses simple is_active flag.",

  "api_contracts": {
    "primary_endpoint": {
      "method": "POST",
      "path": "/anthropic/query",
      "description": "Execute a role-based query against Anthropic API using DB-backed prompt templates",
      "request_schema": {
        "type": "object",
        "required": ["role", "input_format", "payload"],
        "properties": {
          "role": {
            "type": "string",
            "description": "Mentor role identifier (e.g., 'pm_mentor', 'architect_mentor', 'developer_mentor', 'qa_mentor', 'ba_mentor')",
            "examples": ["pm_mentor", "architect_mentor"]
          },
          "input_format": {
            "type": "string",
            "enum": ["plain_text", "json"],
            "description": "Format of the payload: 'plain_text' for PM role, 'json' for all other roles"
          },
          "payload": {
            "type": "string",
            "description": "Input content - either raw text (PM role) or JSON-serialized string (other roles)",
            "maxLength": 51200
          },
          "options": {
            "type": "object",
            "required": false,
            "properties": {
              "model": {
                "type": "string",
                "description": "Override default Claude model",
                "default": "claude-sonnet-4.5-20250929"
              },
              "temperature": {
                "type": "number",
                "minimum": 0.0,
                "maximum": 1.0,
                "description": "Sampling temperature",
                "default": 0.7
              },
              "max_tokens": {
                "type": "integer",
                "minimum": 1,
                "maximum": 8192,
                "description": "Maximum tokens in response",
                "default": 4096
              }
            }
          }
        }
      },
      "response_schema_success": {
        "type": "object",
        "required": ["role", "prompt_id", "prompt_version", "input_format", "input_payload", "raw_response_text", "parsed_json", "parse_error", "token_usage", "latency_ms", "model", "timestamp"],
        "properties": {
          "role": {
            "type": "string",
            "description": "The role that was queried"
          },
          "prompt_id": {
            "type": "string",
            "format": "uuid",
            "description": "Database ID of the prompt template used"
          },
          "prompt_version": {
            "type": "integer",
            "description": "Version number of the prompt template"
          },
          "input_format": {
            "type": "string",
            "enum": ["plain_text", "json"],
            "description": "Format of input that was provided"
          },
          "input_payload": {
            "type": "string",
            "description": "Original payload that was sent (for reference/debugging)"
          },
          "raw_response_text": {
            "type": "string",
            "description": "Unmodified response text from Anthropic API"
          },
          "parsed_json": {
            "type": ["object", "null"],
            "description": "Parsed JSON object if response was valid JSON, null otherwise"
          },
          "parse_error": {
            "type": ["string", "null"],
            "description": "Error message if JSON parsing failed, null if successful"
          },
          "token_usage": {
            "type": "object",
            "required": ["input_tokens", "output_tokens", "total_tokens"],
            "properties": {
              "input_tokens": {
                "type": "integer",
                "description": "Tokens consumed by input (prompt)"
              },
              "output_tokens": {
                "type": "integer",
                "description": "Tokens generated in response"
              },
              "total_tokens": {
                "type": "integer",
                "description": "Sum of input and output tokens"
              }
            }
          },
          "latency_ms": {
            "type": "integer",
            "description": "End-to-end latency in milliseconds for Anthropic API call"
          },
          "model": {
            "type": "string",
            "description": "Actual Claude model used (e.g., 'claude-sonnet-4.5-20250929')"
          },
          "timestamp": {
            "type": "string",
            "format": "date-time",
            "description": "ISO 8601 timestamp when query was executed"
          }
        }
      },
      "response_schema_error": {
        "type": "object",
        "required": ["error_type", "error_message", "role", "timestamp"],
        "properties": {
          "error_type": {
            "type": "string",
            "enum": ["prompt_not_found", "anthropic_api_error", "invalid_input", "configuration_error"],
            "description": "Category of error that occurred"
          },
          "error_message": {
            "type": "string",
            "description": "Human-readable error description"
          },
          "role": {
            "type": "string",
            "description": "Role that was requested (if available)"
          },
          "timestamp": {
            "type": "string",
            "format": "date-time",
            "description": "ISO 8601 timestamp when error occurred"
          },
          "details": {
            "type": "object",
            "description": "Additional error context (optional)"
          }
        }
      },
      "http_status_codes": {
        "200": "Success - includes both successful parsing and parse failures (parse_error populated)",
        "400": "Bad Request - invalid input format, missing role, oversized payload, or prompt_not_found",
        "500": "Internal Server Error - unexpected system failures (DB connection loss, config errors)",
        "503": "Service Unavailable - Anthropic API unreachable or rate limited (retry-able)"
      }
    },
    "validation_rules": {
      "role": "Must be non-empty string, max 50 characters, alphanumeric plus underscore only",
      "input_format": "Must be exactly 'plain_text' or 'json', case-sensitive",
      "payload": "Must be non-empty string, max 50KB (51200 bytes) to prevent token explosion",
      "payload_json_validation": "If input_format='json', payload must be valid JSON string (validated at router before service call)",
      "options": "All optional fields use system defaults if not provided"
    },
    "pydantic_models": {
      "QueryRequest": "Request model with role, input_format, payload, optional options dict",
      "QueryOptions": "Nested model for model, temperature, max_tokens overrides",
      "TokenUsage": "Model with input_tokens, output_tokens, total_tokens (all integers)",
      "QueryResponseEnvelope": "Success response matching response_schema_success exactly",
      "QueryErrorResponse": "Error response matching response_schema_error exactly"
    },
    "json_only_enforcement": "All responses MUST be valid JSON. Router never returns HTML, plain text, or markdown. Even catastrophic errors return JSON error envelope. Content-Type always 'application/json'."
  },

  "repository_design": {
    "database_schema": {
      "table_name": "prompts",
      "columns": [
        {
          "name": "id",
          "type": "UUID",
          "constraints": "PRIMARY KEY DEFAULT gen_random_uuid()",
          "description": "Unique identifier for each prompt record"
        },
        {
          "name": "role",
          "type": "VARCHAR(50)",
          "constraints": "NOT NULL, INDEX",
          "description": "Role identifier (e.g., 'pm_mentor', 'architect_mentor')"
        },
        {
          "name": "version",
          "type": "INTEGER",
          "constraints": "NOT NULL DEFAULT 1",
          "description": "Version number for this prompt template"
        },
        {
          "name": "is_active",
          "type": "BOOLEAN",
          "constraints": "NOT NULL DEFAULT true, INDEX",
          "description": "Whether this is the currently active version for this role"
        },
        {
          "name": "template",
          "type": "TEXT",
          "constraints": "NOT NULL",
          "description": "Full prompt template with {input_payload} placeholder"
        },
        {
          "name": "input_mode",
          "type": "VARCHAR(20)",
          "constraints": "NOT NULL CHECK (input_mode IN ('plain_text', 'json'))",
          "description": "Expected format of input payload"
        },
        {
          "name": "output_contract",
          "type": "TEXT",
          "constraints": "NULL",
          "description": "Optional JSON schema or description of expected output structure"
        },
        {
          "name": "created_at",
          "type": "TIMESTAMP WITH TIME ZONE",
          "constraints": "NOT NULL DEFAULT CURRENT_TIMESTAMP",
          "description": "When this prompt was created"
        },
        {
          "name": "updated_at",
          "type": "TIMESTAMP WITH TIME ZONE",
          "constraints": "NOT NULL DEFAULT CURRENT_TIMESTAMP",
          "description": "Last modification timestamp"
        }
      ],
      "indexes": [
        "CREATE INDEX idx_prompts_role_active ON prompts(role, is_active) WHERE is_active = true",
        "CREATE UNIQUE INDEX idx_prompts_role_version ON prompts(role, version)"
      ],
      "constraints": [
        "Only one is_active=true per role enforced via application logic (repository layer)",
        "Version numbers must be positive integers, gaps allowed"
      ]
    },
    "sqlalchemy_model": {
      "class_name": "Prompt",
      "file_path": "app/orchestrator_api/persistence/models/prompt.py",
      "relationships": "None in v1 (standalone model)",
      "notes": "Uses declarative base, includes __repr__ for debugging, created_at/updated_at use server_default"
    },
    "repository_interface": {
      "class_name": "PromptRepository",
      "file_path": "app/orchestrator_api/persistence/repositories/prompt_repository.py",
      "methods": [
        {
          "name": "get_active_prompt",
          "signature": "def get_active_prompt(self, role: str) -> Prompt | None",
          "description": "Fetch the currently active prompt for a given role",
          "behavior": "Returns None if no active prompt exists for role. Raises exception on DB errors.",
          "query": "SELECT * FROM prompts WHERE role = :role AND is_active = true LIMIT 1"
        },
        {
          "name": "get_prompt_by_id",
          "signature": "def get_prompt_by_id(self, prompt_id: UUID) -> Prompt | None",
          "description": "Fetch specific prompt by ID (for audit/debugging)",
          "behavior": "Returns None if ID not found. Raises exception on DB errors.",
          "query": "SELECT * FROM prompts WHERE id = :id"
        },
        {
          "name": "list_prompts_for_role",
          "signature": "def list_prompts_for_role(self, role: str) -> list[Prompt]",
          "description": "Fetch all versions of prompts for a role (for future version management)",
          "behavior": "Returns empty list if no prompts exist. Ordered by version DESC.",
          "query": "SELECT * FROM prompts WHERE role = :role ORDER BY version DESC"
        }
      ],
      "dependencies": "Requires SQLAlchemy session (injected via constructor or dependency injection)",
      "error_handling": "DB connection errors propagate as SQLAlchemyError (caught by service layer). Missing prompt returns None (not an exception).",
      "testability": "Repository is mockable. Tests use in-memory SQLite or DB fixtures."
    },
    "seeding_strategy": {
      "approach": "Alembic migration or standalone seed script",
      "seed_data": "Five mentor roles (pm_mentor, architect_mentor, ba_mentor, developer_mentor, qa_mentor) with is_active=true, version=1",
      "template_format": "Templates stored as plain text with {input_payload} placeholder. Example: 'You are the PM Mentor. Given this Epic:\\n\\n{input_payload}\\n\\nProduce...'",
      "location": "Seed files in app/orchestrator_api/persistence/seeds/ or inline in migration",
      "notes": "Templates must match PM Mentor document structure. PM role uses plain_text input_mode, others use json."
    },
    "zero_hardcoded_prompts": "No prompt text appears in Python code outside of seed files. All runtime prompt retrieval goes through PromptRepository. Service layer never constructs prompts from literals."
  },

  "anthropic_integration": {
    "client_adapter": {
      "class_name": "AnthropicClientAdapter",
      "file_path": "app/orchestrator_api/clients/anthropic_client.py",
      "responsibilities": [
        "Wrap Anthropic SDK (anthropic Python library)",
        "Manage API key from environment/config",
        "Execute single-turn message calls",
        "Capture response metadata (tokens, latency, model)",
        "Return structured result object (not raw SDK response)"
      ],
      "interface": {
        "method": "call",
        "signature": "async def call(self, *, prompt: str, system: str | None = None, model: str, temperature: float, max_tokens: int) -> AnthropicCallResult",
        "parameters": {
          "prompt": "User message content (the rendered prompt with payload injected)",
          "system": "Optional system message (future use for role-level instructions)",
          "model": "Claude model identifier (e.g., 'claude-sonnet-4.5-20250929')",
          "temperature": "Sampling temperature (0.0-1.0)",
          "max_tokens": "Maximum tokens to generate"
        },
        "returns": "AnthropicCallResult object with content, usage, model, latency_ms"
      }
    },
    "result_structure": {
      "class_name": "AnthropicCallResult",
      "fields": [
        {
          "name": "content",
          "type": "str",
          "description": "Raw text response from Claude"
        },
        {
          "name": "model",
          "type": "str",
          "description": "Actual model used (may differ from request if fallback occurs)"
        },
        {
          "name": "input_tokens",
          "type": "int",
          "description": "Tokens consumed by prompt"
        },
        {
          "name": "output_tokens",
          "type": "int",
          "description": "Tokens generated in response"
        },
        {
          "name": "latency_ms",
          "type": "int",
          "description": "Milliseconds elapsed for API call"
        }
      ]
    },
    "configuration": {
      "api_key": "Loaded from environment variable ANTHROPIC_API_KEY, fails fast if missing",
      "default_model": "claude-sonnet-4.5-20250929 (configurable via ANTHROPIC_MODEL_NAME env var)",
      "default_timeout": "60 seconds for API call (configurable via ANTHROPIC_TIMEOUT_SECONDS)",
      "retry_policy": "No retries in v1 (fail fast). Future enhancement: exponential backoff for 429/503.",
      "notes": "SDK handles connection pooling and keepalive. Adapter does not cache results."
    },
    "json_enforcement": {
      "strategy": "System prompt instructs Claude to return ONLY valid JSON with no markdown fences",
      "fallback": "If Claude returns markdown-wrapped JSON (```json...```), adapter does NOT strip fences - service layer handles this during parsing",
      "validation": "Adapter does not validate JSON structure, only returns raw content. Service layer attempts parse."
    },
    "error_handling": {
      "anthropic_sdk_errors": [
        {
          "error_type": "anthropic.APIConnectionError",
          "handling": "Catch and re-raise as AnthropicClientError with 'connection_failed' type",
          "http_mapping": "503 Service Unavailable"
        },
        {
          "error_type": "anthropic.RateLimitError",
          "handling": "Catch and re-raise as AnthropicClientError with 'rate_limited' type",
          "http_mapping": "503 Service Unavailable (retry-able)"
        },
        {
          "error_type": "anthropic.AuthenticationError",
          "handling": "Catch and re-raise as AnthropicClientError with 'authentication_failed' type",
          "http_mapping": "500 Internal Server Error (config issue)"
        },
        {
          "error_type": "anthropic.APITimeoutError",
          "handling": "Catch and re-raise as AnthropicClientError with 'timeout' type",
          "http_mapping": "503 Service Unavailable"
        },
        {
          "error_type": "anthropic.BadRequestError",
          "handling": "Catch and re-raise as AnthropicClientError with 'invalid_request' type",
          "http_mapping": "500 Internal Server Error (indicates prompt construction bug)"
        }
      ],
      "custom_exception": {
        "class_name": "AnthropicClientError",
        "fields": ["error_type", "message", "original_exception"],
        "purpose": "Isolate SDK exceptions from service layer, enable testing without SDK mocks"
      }
    },
    "logging": {
      "log_on_call_start": "role, model, estimated_input_size",
      "log_on_call_success": "role, model, input_tokens, output_tokens, latency_ms",
      "log_on_call_error": "role, model, error_type, error_message",
      "exclude_from_logs": "Full prompt text (PII/privacy concern), full response text (logged at service layer if needed), API key",
      "log_level": "INFO for success, WARNING for rate limit, ERROR for other failures"
    },
    "token_cost_instrumentation": {
      "capture_fields": "input_tokens, output_tokens from Anthropic SDK usage object",
      "calculation": "total_tokens = input_tokens + output_tokens (computed in service layer)",
      "future_enhancement": "Cost calculation based on model pricing (deferred to observability epic)"
    },
    "testability": {
      "mock_strategy": "AnthropicClientAdapter accepts optional client parameter for injection of mock Anthropic SDK client",
      "test_fixtures": "Provide sample AnthropicCallResult objects for common scenarios (success, rate limit, timeout)",
      "notes": "Tests should not call real Anthropic API. Use pytest fixtures or mocks."
    }
  },

  "data_flow": "Request enters via POST /anthropic/query with role/payload → Router validates request schema (Pydantic) and payload size → Router calls QueryEngineService.execute() → Service calls PromptRepository.get_active_prompt(role) → Repository queries DB and returns Prompt object (or None) → Service checks if prompt exists (if not, raises PromptNotFoundError → 400) → Service renders template with payload using simple string substitution: template.replace('{input_payload}', payload) → Service calls AnthropicClientAdapter.call(prompt=rendered, model=..., temperature=..., max_tokens=...) → Adapter constructs Anthropic SDK message request with user message → Adapter executes API call and measures latency → Adapter captures response content and usage metadata → Adapter returns AnthropicCallResult → Service receives result and attempts json.loads(result.content) → If parse succeeds: parsed_json populated, parse_error=None → If parse fails: parsed_json=None, parse_error contains exception message → Service constructs QueryResponseEnvelope with all fields → Service returns envelope to router → Router serializes envelope to JSON and returns HTTP 200 → Client receives JSON response. Error paths: PromptNotFoundError triggers HTTP 400 error response; AnthropicClientError triggers HTTP 503 response with error envelope; unexpected exceptions trigger HTTP 500 with sanitized error envelope.",

  "non_functional_requirements": {
    "performance": {
      "latency_target": "P95 end-to-end latency < 5 seconds for typical queries (driven by Anthropic API, not our code)",
      "throughput": "Support 10 concurrent requests without degradation (FastAPI async handles concurrency)",
      "database_query_time": "PromptRepository.get_active_prompt() < 50ms (indexed query on role+is_active)",
      "notes": "No caching in v1. Future optimization: cache active prompts in-memory with TTL."
    },
    "reliability": {
      "error_rate_target": "< 1% error rate excluding Anthropic API failures (our code should not fail)",
      "anthropic_api_availability": "Dependent on Anthropic SLA (99.9% typical), we do not control this",
      "database_availability": "Assumes PostgreSQL is highly available (RDS multi-AZ or equivalent)",
      "graceful_degradation": "If DB unavailable, return 500 with clear error. If Anthropic unavailable, return 503."
    },
    "security": {
      "api_key_protection": "ANTHROPIC_API_KEY stored in environment, never logged or returned in responses",
      "input_validation": "Max payload size 50KB enforced at router to prevent abuse",
      "output_sanitization": "Error messages do not expose internal paths, DB schemas, or stack traces",
      "audit_logging": "All queries logged with role, timestamp, token usage (but not full payload/response)",
      "notes": "No authentication on /anthropic/query endpoint in v1 (internal service only)"
    },
    "observability": {
      "structured_logging": "JSON-formatted logs with fields: timestamp, level, role, latency_ms, tokens, error_type",
      "metrics": "Expose via future observability system: query_count, error_count, p95_latency, token_usage",
      "tracing": "Include correlation_id in logs (generated per request, included in response envelope)",
      "log_retention": "Logs stored for 30 days minimum (configurable)"
    },
    "maintainability": {
      "code_organization": "Clear separation: routers/ for API, services/ for orchestration, repositories/ for DB, clients/ for external APIs",
      "type_safety": "All functions fully type-annotated (Python 3.11+ with strict mypy)",
      "test_coverage": "> 80% line coverage, 100% coverage of critical paths (prompt loading, envelope construction)",
      "documentation": "Inline docstrings for all public methods, README with architecture diagram"
    },
    "scalability": {
      "horizontal_scaling": "Service is stateless, can run multiple instances behind load balancer",
      "database_connection_pooling": "SQLAlchemy connection pool sized for expected concurrency (10-20 connections)",
      "anthropic_rate_limits": "Respect Anthropic tier limits (no built-in rate limiting in v1, assumes low traffic)",
      "future_considerations": "Add request queuing if Anthropic rate limits become bottleneck"
    }
  },

  "open_questions": [
    "Should we implement prompt template caching in-memory to reduce DB load, or is one query per request acceptable for v1?",
    "What is the expected request volume for this service? Do we need connection pooling tuning or rate limiting?",
    "Should correlation_id be generated by the router or passed in by the caller?",
    "Do we want to log full prompts and responses for debugging, and if so, how do we handle PII/privacy concerns?",
    "Should we validate that the loaded prompt template contains the {input_payload} placeholder before calling Anthropic?",
    "What is the desired behavior if Anthropic returns a response that exceeds max_tokens (truncated JSON)? Return as-is with parse_error?",
    "Should we implement circuit breaker pattern for Anthropic API calls to prevent cascading failures?",
    "Do we need health check endpoint (GET /health) for load balancer or monitoring?",
    "Should QueryResponseEnvelope include correlation_id or request_id for distributed tracing?",
    "What is the migration strategy for updating active prompts in production? Direct DB update or future admin API?"
  ]
}
```