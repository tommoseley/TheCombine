{
  "files": [
    {
      "path": "scripts/setup.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Environment setup script for The Combine.\"\"\"\nimport sys\nimport subprocess\nimport venv\nfrom pathlib import Path\n\nROOT = Path(__file__).parent.parent\nVENV_PATH = ROOT / \"venv\"\n\ndef main():\n    print(\"ðŸš€ Setting up The Combine environment...\")\n    \n    if not create_venv():\n        return 1\n    \n    if not install_dependencies():\n        return 1\n    \n    if not initialize_database():\n        return 1\n    \n    if not run_tests():\n        return 1\n    \n    print(\"âœ… Setup complete!\")\n    print(\"Run: ./run.sh (or run.ps1 on Windows) to start server\")\n    return 0\n\ndef create_venv():\n    print(\"Creating virtual environment...\")\n    try:\n        venv.create(VENV_PATH, with_pip=True)\n        print(\"âœ… Virtual environment created\")\n        return True\n    except Exception as e:\n        print(f\"âŒ Failed to create venv: {e}\", file=sys.stderr)\n        return False\n\ndef install_dependencies():\n    print(\"Installing dependencies...\")\n    pip_path = VENV_PATH / \"bin\" / \"pip\" if sys.platform != \"win32\" else VENV_PATH / \"Scripts\" / \"pip.exe\"\n    requirements = ROOT / \"requirements.txt\"\n    \n    try:\n        subprocess.run(\n            [str(pip_path), \"install\", \"-r\", str(requirements)],\n            check=True,\n            capture_output=True\n        )\n        print(\"âœ… Dependencies installed\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"âŒ Failed to install dependencies: {e.stderr.decode()}\", file=sys.stderr)\n        return False\n\ndef initialize_database():\n    print(\"Initializing database...\")\n    python_path = VENV_PATH / \"bin\" / \"python\" if sys.platform != \"win32\" else VENV_PATH / \"Scripts\" / \"python.exe\"\n    init_script = ROOT / \"scripts\" / \"init_db.py\"\n    \n    try:\n        subprocess.run(\n            [str(python_path), str(init_script)],\n            check=True,\n            capture_output=True,\n            cwd=ROOT\n        )\n        print(\"âœ… Database initialized\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"âŒ Failed to initialize database: {e.stderr.decode()}\", file=sys.stderr)\n        return False\n\ndef run_tests():\n    print(\"Running tests...\")\n    python_path = VENV_PATH / \"bin\" / \"python\" if sys.platform != \"win32\" else VENV_PATH / \"Scripts\" / \"python.exe\"\n    \n    try:\n        subprocess.run(\n            [str(python_path), \"-m\", \"pytest\", \"tests/\", \"-v\"],\n            check=True,\n            capture_output=True,\n            cwd=ROOT\n        )\n        print(\"âœ… All tests passed\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"âŒ Tests failed: {e.stderr.decode()}\", file=sys.stderr)\n        return False\n\nif __name__ == \"__main__\":\n    sys.exit(main())",
      "imports": ["sys", "subprocess", "venv", "pathlib.Path"],
      "classes": [],
      "functions": ["main", "create_venv", "install_dependencies", "initialize_database", "run_tests"]
    },
    {
      "path": "scripts/init_db.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Database initialization script.\"\"\"\nimport sys\nfrom pathlib import Path\n\nROOT = Path(__file__).parent.parent\nsys.path.insert(0, str(ROOT))\n\nfrom sqlalchemy import create_engine, text\nfrom config import Settings\nfrom app.orchestrator_api.persistence.models import Base\n\ndef main():\n    print(\"ðŸ”§ Initializing database...\")\n    settings = Settings()\n    \n    if not create_tables(settings.DATABASE_URL):\n        return 1\n    \n    if not run_migrations(settings.DATABASE_URL):\n        return 1\n    \n    if not seed_data(settings.DATABASE_URL):\n        return 1\n    \n    print(\"âœ… Database initialization complete\")\n    return 0\n\ndef create_tables(database_url: str) -> bool:\n    print(\"Creating tables...\")\n    try:\n        engine = create_engine(database_url)\n        Base.metadata.create_all(engine)\n        print(\"âœ… Tables created\")\n        return True\n    except Exception as e:\n        print(f\"âŒ Failed to create tables: {e}\", file=sys.stderr)\n        return False\n\ndef run_migrations(database_url: str) -> bool:\n    print(\"Running migrations...\")\n    try:\n        from app.orchestrator_api.persistence.migrations import migration_002_add_token_tracking\n        migration_002_add_token_tracking.upgrade(database_url)\n        print(\"âœ… Migrations complete\")\n        return True\n    except Exception as e:\n        print(f\"âŒ Migration failed: {e}\", file=sys.stderr)\n        return False\n\ndef seed_data(database_url: str) -> bool:\n    print(\"Seeding data...\")\n    try:\n        engine = create_engine(database_url)\n        with engine.connect() as conn:\n            result = conn.execute(text(\"SELECT COUNT(*) FROM role_prompts\"))\n            count = result.scalar()\n            if count > 0:\n                print(\"âœ… Data already seeded, skipping\")\n                return True\n        \n        from scripts import seed_prompts, seed_phases\n        seed_prompts.seed(database_url)\n        seed_phases.seed(database_url)\n        print(\"âœ… Data seeded\")\n        return True\n    except Exception as e:\n        print(f\"âŒ Seeding failed: {e}\", file=sys.stderr)\n        return False\n\nif __name__ == \"__main__\":\n    sys.exit(main())",
      "imports": ["sys", "pathlib.Path", "sqlalchemy", "config.Settings", "app.orchestrator_api.persistence.models.Base"],
      "classes": [],
      "functions": ["main", "create_tables", "run_migrations", "seed_data"]
    },
    {
      "path": "scripts/run.py",
      "content": "#!/usr/bin/env python3\n\"\"\"Server startup script.\"\"\"\nimport sys\nfrom pathlib import Path\n\nROOT = Path(__file__).parent.parent\nsys.path.insert(0, str(ROOT))\n\nimport os\nfrom dotenv import load_dotenv\nimport uvicorn\nfrom config import Settings\n\ndef main():\n    print(\"ðŸš€ Starting The Combine API...\")\n    \n    env_file = ROOT / \".env\"\n    if env_file.exists():\n        load_dotenv(env_file)\n        print(f\"âœ… Loaded environment from {env_file}\")\n    \n    try:\n        settings = Settings()\n    except Exception as e:\n        print(f\"âŒ Configuration error: {e}\", file=sys.stderr)\n        return 1\n    \n    os.environ[\"WORKBENCH_DATA_DRIVEN_ORCHESTRATION\"] = \"true\"\n    \n    print(f\"   Database: {settings.DATABASE_URL}\")\n    print(f\"   Feature flag: DATA_DRIVEN_ORCHESTRATION=true\")\n    print(f\"   API docs: http://localhost:8000/docs\")\n    print(f\"   Health: http://localhost:8000/health\")\n    print()\n    \n    try:\n        uvicorn.run(\n            \"app.orchestrator_api.main:app\",\n            host=\"127.0.0.1\",\n            port=8000,\n            reload=True,\n            log_level=\"info\"\n        )\n        return 0\n    except Exception as e:\n        print(f\"âŒ Server failed to start: {e}\", file=sys.stderr)\n        return 1\n\nif __name__ == \"__main__\":\n    sys.exit(main())",
      "imports": ["sys", "pathlib.Path", "os", "dotenv.load_dotenv", "uvicorn", "config.Settings"],
      "classes": [],
      "functions": ["main"]
    },
    {
      "path": "app/orchestrator_api/routers/health.py",
      "content": "\"\"\"Health check router.\"\"\"\nfrom fastapi import APIRouter\n\nrouter = APIRouter(tags=[\"health\"])\n\n@router.get(\"/health\")\ndef health_check():\n    \"\"\"Health check endpoint.\n    \n    Returns:\n        dict: Service status\n    \"\"\"\n    return {\n        \"status\": \"ok\",\n        \"service\": \"The Combine Orchestrator API\",\n        \"version\": \"175C\"\n    }",
      "imports": ["fastapi.APIRouter"],
      "classes": [],
      "functions": ["health_check"]
    },
    {
      "path": "app/orchestrator_api/schemas/artifacts.py",
      "content": "\"\"\"Pydantic schemas for pipeline artifacts.\"\"\"\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass Story(BaseModel):\n    \"\"\"User story within an epic.\"\"\"\n    id: str = Field(pattern=r\"^STORY-\\d+$\")\n    title: str = Field(max_length=100)\n    user_story: str\n    acceptance_criteria: List[str]\n    estimate_hours: int = Field(gt=0, le=40)\n    priority: str = Field(pattern=r\"^(critical|high|medium|low)$\")\n\nclass EpicSpec(BaseModel):\n    \"\"\"PM phase output schema.\"\"\"\n    epic_id: str\n    title: str = Field(max_length=100)\n    goal: str = Field(max_length=500)\n    success_criteria: List[str] = Field(min_length=1)\n    stories: List[Story] = Field(min_length=1)\n    out_of_scope: List[str] = []\n    risks: List[str] = []\n    total_estimate_hours: int = Field(gt=0)\n\nclass MethodSpec(BaseModel):\n    \"\"\"Method specification.\"\"\"\n    name: str\n    purpose: str\n    params: List[dict]\n    returns: str\n    raises: List[str] = []\n\nclass ComponentSpec(BaseModel):\n    \"\"\"Component specification.\"\"\"\n    name: str\n    purpose: str\n    file_path: str\n    responsibilities: List[str]\n    dependencies: List[str]\n    public_interface: List[MethodSpec]\n    error_handling: List[str]\n    test_count: int\n\nclass ADR(BaseModel):\n    \"\"\"Architecture Decision Record.\"\"\"\n    id: str\n    title: str\n    decision: str\n    rationale: str\n    consequences: dict\n\nclass ArchitectureSpec(BaseModel):\n    \"\"\"Architect phase output schema.\"\"\"\n    epic_id: str\n    components: List[ComponentSpec]\n    adrs: List[ADR]\n    test_strategy: dict\n    acceptance_criteria: List[str]\n\nclass FileSpec(BaseModel):\n    \"\"\"File specification.\"\"\"\n    path: str\n    content: str\n    imports: List[str] = []\n    classes: List[str] = []\n    functions: List[str] = []\n\nclass CodeDeliverable(BaseModel):\n    \"\"\"Developer phase output schema.\"\"\"\n    files: List[FileSpec]\n    test_files: List[FileSpec]\n    dependencies: List[str]\n    migration_scripts: List[FileSpec] = []\n\nclass IssueSpec(BaseModel):\n    \"\"\"QA issue specification.\"\"\"\n    id: str\n    severity: str = Field(pattern=r\"^(critical|high|medium|low)$\")\n    category: str\n    description: str\n    location: str\n    fix_required: bool\n\nclass QAReport(BaseModel):\n    \"\"\"QA phase output schema.\"\"\"\n    passed: bool\n    issues: List[IssueSpec]\n    test_results: dict\n    recommendations: List[str]",
      "imports": ["pydantic.BaseModel", "pydantic.Field", "typing"],
      "classes": ["Story", "EpicSpec", "MethodSpec", "ComponentSpec", "ADR", "ArchitectureSpec", "FileSpec", "CodeDeliverable", "IssueSpec", "QAReport"],
      "functions": []
    },
    {
      "path": "app/orchestrator_api/utils/pricing.py",
      "content": "\"\"\"LLM API pricing utilities.\"\"\"\nimport logging\nfrom config import Settings\n\nlogger = logging.getLogger(__name__)\n\ndef calculate_cost(input_tokens: int, output_tokens: int, model: str = \"claude-sonnet-4\") -> float:\n    \"\"\"Calculate USD cost from token usage.\n    \n    Args:\n        input_tokens: Number of input tokens\n        output_tokens: Number of output tokens\n        model: Model identifier\n    \n    Returns:\n        Cost in USD (6 decimal places)\n    \"\"\"\n    if input_tokens < 0 or output_tokens < 0:\n        logger.warning(f\"Invalid token counts: input={input_tokens}, output={output_tokens}\")\n        return 0.0\n    \n    settings = Settings()\n    \n    input_price_per_mtk = settings.ANTHROPIC_INPUT_PRICE_PER_MTK\n    output_price_per_mtk = settings.ANTHROPIC_OUTPUT_PRICE_PER_MTK\n    \n    if \"claude\" not in model.lower():\n        logger.warning(f\"Unknown model '{model}', using default Anthropic pricing\")\n    \n    input_cost = (input_tokens / 1_000_000) * input_price_per_mtk\n    output_cost = (output_tokens / 1_000_000) * output_price_per_mtk\n    total_cost = input_cost + output_cost\n    \n    return round(total_cost, 6)",
      "imports": ["logging", "config.Settings"],
      "classes": [],
      "functions": ["calculate_cost"]
    },
    {
      "path": "app/orchestrator_api/services/usage_recorder.py",
      "content": "\"\"\"Prompt usage recording service.\"\"\"\nfrom dataclasses import dataclass\nimport logging\nfrom app.orchestrator_api.persistence.repositories.pipeline_prompt_usage_repository import (\n    PipelinePromptUsageRepository\n)\nfrom app.orchestrator_api.utils.pricing import calculate_cost\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass UsageRecord:\n    \"\"\"Prompt usage data to record.\"\"\"\n    pipeline_id: str\n    prompt_id: str\n    role_name: str\n    phase_name: str\n    input_tokens: int = 0\n    output_tokens: int = 0\n    execution_time_ms: int = 0\n    model: str = \"claude-sonnet-4-20250514\"\n\nclass UsageRecorder:\n    \"\"\"Record prompt usage to audit trail.\"\"\"\n    \n    def __init__(self, repo: PipelinePromptUsageRepository):\n        self._repo = repo\n    \n    def record_usage(self, usage: UsageRecord) -> bool:\n        \"\"\"Record prompt usage with token counts and cost.\n        \n        Args:\n            usage: Usage record with token data\n        \n        Returns:\n            True if recorded successfully, False otherwise\n        \"\"\"\n        try:\n            cost_usd = calculate_cost(\n                usage.input_tokens,\n                usage.output_tokens,\n                usage.model\n            )\n            \n            usage_id = self._repo.create(\n                pipeline_id=usage.pipeline_id,\n                prompt_id=usage.prompt_id,\n                role_name=usage.role_name,\n                phase_name=usage.phase_name,\n                input_tokens=usage.input_tokens,\n                output_tokens=usage.output_tokens,\n                cost_usd=cost_usd,\n                model=usage.model,\n                execution_time_ms=usage.execution_time_ms\n            )\n            \n            logger.debug(\n                f\"Recorded usage: {usage_id}, \"\n                f\"tokens={usage.input_tokens + usage.output_tokens}, \"\n                f\"cost=${cost_usd:.6f}\"\n            )\n            \n            return True\n            \n        except Exception as e:\n            logger.warning(\n                \"Usage record failure\",\n                extra={\n                    \"event\": \"usage_record_failure\",\n                    \"pipeline_id\": usage.pipeline_id,\n                    \"phase_name\": usage.phase_name,\n                    \"role_name\": usage.role_name,\n                    \"prompt_id\": usage.prompt_id,\n                    \"input_tokens\": usage.input_tokens,\n                    \"output_tokens\": usage.output_tokens,\n                    \"error\": str(e)\n                }\n            )\n            return False",
      "imports": ["dataclasses.dataclass", "logging", "app.orchestrator_api.persistence.repositories.pipeline_prompt_usage_repository.PipelinePromptUsageRepository", "app.orchestrator_api.utils.pricing.calculate_cost"],
      "classes": ["UsageRecord", "UsageRecorder"],
      "functions": ["record_usage"]
    },
    {
      "path": "app/orchestrator_api/persistence/migrations/002_add_token_tracking.py",
      "content": "\"\"\"Migration: Add token tracking columns to pipeline_prompt_usage.\"\"\"\nfrom sqlalchemy import create_engine, text\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef upgrade(database_url: str):\n    \"\"\"Add token tracking columns.\"\"\"\n    engine = create_engine(database_url)\n    \n    with engine.connect() as conn:\n        try:\n            logger.info(\"Adding input_tokens column...\")\n            conn.execute(text(\n                \"ALTER TABLE pipeline_prompt_usage \"\n                \"ADD COLUMN input_tokens INTEGER DEFAULT 0\"\n            ))\n            conn.commit()\n        except Exception as e:\n            if \"duplicate column\" not in str(e).lower():\n                raise\n            logger.info(\"Column input_tokens already exists\")\n        \n        try:\n            logger.info(\"Adding output_tokens column...\")\n            conn.execute(text(\n                \"ALTER TABLE pipeline_prompt_usage \"\n                \"ADD COLUMN output_tokens INTEGER DEFAULT 0\"\n            ))\n            conn.commit()\n        except Exception as e:\n            if \"duplicate column\" not in str(e).lower():\n                raise\n            logger.info(\"Column output_tokens already exists\")\n        \n        try:\n            logger.info(\"Adding cost_usd column...\")\n            conn.execute(text(\n                \"ALTER TABLE pipeline_prompt_usage \"\n                \"ADD COLUMN cost_usd DECIMAL(10, 6) DEFAULT 0.00\"\n            ))\n            conn.commit()\n        except Exception as e:\n            if \"duplicate column\" not in str(e).lower():\n                raise\n            logger.info(\"Column cost_usd already exists\")\n        \n        try:\n            logger.info(\"Adding model column...\")\n            conn.execute(text(\n                \"ALTER TABLE pipeline_prompt_usage \"\n                \"ADD COLUMN model VARCHAR(64) DEFAULT 'claude-sonnet-4-20250514'\"\n            ))\n            conn.commit()\n        except Exception as e:\n            if \"duplicate column\" not in str(e).lower():\n                raise\n            logger.info(\"Column model already exists\")\n        \n        try:\n            logger.info(\"Adding execution_time_ms column...\")\n            conn.execute(text(\n                \"ALTER TABLE pipeline_prompt_usage \"\n                \"ADD COLUMN execution_time_ms INTEGER DEFAULT 0\"\n            ))\n            conn.commit()\n        except Exception as e:\n            if \"duplicate column\" not in str(e).lower():\n                raise\n            logger.info(\"Column execution_time_ms already exists\")\n        \n        try:\n            logger.info(\"Creating index on pipeline_id and phase_name...\")\n            conn.execute(text(\n                \"CREATE INDEX IF NOT EXISTS idx_usage_pipeline_phase \"\n                \"ON pipeline_prompt_usage(pipeline_id, phase_name)\"\n            ))\n            conn.commit()\n        except Exception as e:\n            logger.warning(f\"Index creation warning: {e}\")\n    \n    logger.info(\"âœ… Migration 002 complete\")\n\ndef downgrade(database_url: str):\n    \"\"\"Remove token tracking columns.\"\"\"\n    engine = create_engine(database_url)\n    \n    with engine.connect() as conn:\n        conn.execute(text(\"DROP INDEX IF EXISTS idx_usage_pipeline_phase\"))\n        conn.execute(text(\"ALTER TABLE pipeline_prompt_usage DROP COLUMN execution_time_ms\"))\n        conn.execute(text(\"ALTER TABLE pipeline_prompt_usage DROP COLUMN model\"))\n        conn.execute(text(\"ALTER TABLE pipeline_prompt_usage DROP COLUMN cost_usd\"))\n        conn.execute(text(\"ALTER TABLE pipeline_prompt_usage DROP COLUMN output_tokens\"))\n        conn.execute(text(\"ALTER TABLE pipeline_prompt_usage DROP COLUMN input_tokens\"))\n        conn.commit()\n    \n    logger.info(\"âœ… Migration 002 rolled back\")",
      "imports": ["sqlalchemy", "logging"],
      "classes": [],
      "functions": ["upgrade", "downgrade"]
    }
  ],
  "test_files": [
    {
      "path": "tests/unit/schemas/test_artifact_schemas.py",
      "content": "\"\"\"Tests for artifact schemas.\"\"\"\nimport pytest\nfrom pydantic import ValidationError\nfrom app.orchestrator_api.schemas.artifacts import (\n    EpicSpec, Story, ArchitectureSpec, ComponentSpec,\n    CodeDeliverable, FileSpec, QAReport, IssueSpec\n)\n\nclass TestEpicSpec:\n    def test_valid_epic(self):\n        data = {\n            \"epic_id\": \"EPIC-001\",\n            \"title\": \"Test Epic\",\n            \"goal\": \"Test goal\",\n            \"success_criteria\": [\"Criterion 1\"],\n            \"stories\": [\n                {\n                    \"id\": \"STORY-1\",\n                    \"title\": \"Test Story\",\n                    \"user_story\": \"As a user\",\n                    \"acceptance_criteria\": [\"AC1\"],\n                    \"estimate_hours\": 8,\n                    \"priority\": \"high\"\n                }\n            ],\n            \"total_estimate_hours\": 8\n        }\n        epic = EpicSpec(**data)\n        assert epic.epic_id == \"EPIC-001\"\n        assert len(epic.stories) == 1\n    \n    def test_missing_required_field(self):\n        data = {\"epic_id\": \"EPIC-001\", \"title\": \"Test\"}\n        with pytest.raises(ValidationError):\n            EpicSpec(**data)\n    \n    def test_invalid_story_id(self):\n        data = {\n            \"epic_id\": \"EPIC-001\",\n            \"title\": \"Test\",\n            \"goal\": \"Goal\",\n            \"success_criteria\": [\"SC1\"],\n            \"stories\": [\n                {\n                    \"id\": \"BAD-ID\",\n                    \"title\": \"Story\",\n                    \"user_story\": \"As a user\",\n                    \"acceptance_criteria\": [\"AC1\"],\n                    \"estimate_hours\": 8,\n                    \"priority\": \"high\"\n                }\n            ],\n            \"total_estimate_hours\": 8\n        }\n        with pytest.raises(ValidationError):\n            EpicSpec(**data)\n    \n    def test_invalid_priority(self):\n        data = {\n            \"epic_id\": \"EPIC-001\",\n            \"title\": \"Test\",\n            \"goal\": \"Goal\",\n            \"success_criteria\": [\"SC1\"],\n            \"stories\": [\n                {\n                    \"id\": \"STORY-1\",\n                    \"title\": \"Story\",\n                    \"user_story\": \"As a user\",\n                    \"acceptance_criteria\": [\"AC1\"],\n                    \"estimate_hours\": 8,\n                    \"priority\": \"urgent\"\n                }\n            ],\n            \"total_estimate_hours\": 8\n        }\n        with pytest.raises(ValidationError):\n            EpicSpec(**data)\n\nclass TestArchitectureSpec:\n    def test_valid_architecture(self):\n        data = {\n            \"epic_id\": \"EPIC-001\",\n            \"components\": [\n                {\n                    \"name\": \"TestComponent\",\n                    \"purpose\": \"Test\",\n                    \"file_path\": \"test.py\",\n                    \"responsibilities\": [\"Do thing\"],\n                    \"dependencies\": [],\n                    \"public_interface\": [],\n                    \"error_handling\": [],\n                    \"test_count\": 5\n                }\n            ],\n            \"adrs\": [],\n            \"test_strategy\": {},\n            \"acceptance_criteria\": [\"AC1\"]\n        }\n        arch = ArchitectureSpec(**data)\n        assert arch.epic_id == \"EPIC-001\"\n        assert len(arch.components) == 1\n\nclass TestCodeDeliverable:\n    def test_valid_code_deliverable(self):\n        data = {\n            \"files\": [{\"path\": \"test.py\", \"content\": \"print('hello')\"}],\n            \"test_files\": [],\n            \"dependencies\": [\"pytest\"]\n        }\n        code = CodeDeliverable(**data)\n        assert len(code.files) == 1\n\nclass TestQAReport:\n    def test_valid_qa_report(self):\n        data = {\n            \"passed\": False,\n            \"issues\": [\n                {\n                    \"id\": \"QA-1\",\n                    \"severity\": \"high\",\n                    \"category\": \"code\",\n                    \"description\": \"Issue\",\n                    \"location\": \"test.py:10\",\n                    \"fix_required\": True\n                }\n            ],\n            \"test_results\": {},\n            \"recommendations\": [\"Fix issues\"]\n        }\n        report = QAReport(**data)\n        assert report.passed is False"
    },
    {
      "path": "tests/unit/utils/test_pricing.py",
      "content": "\"\"\"Tests for pricing utility.\"\"\"\nimport pytest\nfrom app.orchestrator_api.utils.pricing import calculate_cost\n\nclass TestCalculateCost:\n    def test_zero_tokens(self):\n        cost = calculate_cost(0, 0)\n        assert cost == 0.0\n    \n    def test_typical_usage(self):\n        cost = calculate_cost(10000, 2000)\n        expected = (10000 / 1_000_000 * 3.0) + (2000 / 1_000_000 * 15.0)\n        assert cost == round(expected, 6)\n    \n    def test_negative_tokens(self):\n        cost = calculate_cost(-100, 200)\n        assert cost == 0.0\n    \n    def test_unknown_model_uses_default(self):\n        cost = calculate_cost(1000, 500, \"gpt-4\")\n        assert cost > 0.0"
    },
    {
      "path": "tests/unit/services/test_usage_recorder_tokens.py",
      "content": "\"\"\"Tests for UsageRecorder with token tracking.\"\"\"\nimport pytest\nfrom unittest.mock import Mock\nfrom app.orchestrator_api.services.usage_recorder import UsageRecorder, UsageRecord\n\nclass TestUsageRecorderTokens:\n    def test_records_token_counts(self):\n        repo = Mock()\n        repo.create.return_value = \"usage_123\"\n        recorder = UsageRecorder(repo)\n        \n        usage = UsageRecord(\n            pipeline_id=\"pip_test\",\n            prompt_id=\"rp_123\",\n            role_name=\"pm\",\n            phase_name=\"pm_phase\",\n            input_tokens=1000,\n            output_tokens=500\n        )\n        \n        result = recorder.record_usage(usage)\n        assert result is True\n        repo.create.assert_called_once()\n        call_args = repo.create.call_args[1]\n        assert call_args[\"input_tokens\"] == 1000\n        assert call_args[\"output_tokens\"] == 500\n        assert \"cost_usd\" in call_args\n    \n    def test_calculates_cost(self):\n        repo = Mock()\n        repo.create.return_value = \"usage_123\"\n        recorder = UsageRecorder(repo)\n        \n        usage = UsageRecord(\n            pipeline_id=\"pip_test\",\n            prompt_id=\"rp_123\",\n            role_name=\"pm\",\n            phase_name=\"pm_phase\",\n            input_tokens=10000,\n            output_tokens=2000\n        )\n        \n        recorder.record_usage(usage)\n        call_args = repo.create.call_args[1]\n        expected_cost = (10000 / 1_000_000 * 3.0) + (2000 / 1_000_000 * 15.0)\n        assert call_args[\"cost_usd\"] == round(expected_cost, 6)\n    \n    def test_never_raises(self):\n        repo = Mock()\n        repo.create.side_effect = Exception(\"DB error\")\n        recorder = UsageRecorder(repo)\n        \n        usage = UsageRecord(\n            pipeline_id=\"pip_test\",\n            prompt_id=\"rp_123\",\n            role_name=\"pm\",\n            phase_name=\"pm_phase\"\n        )\n        \n        result = recorder.record_usage(usage)\n        assert result is False"
    },
    {
      "path": "tests/unit/migrations/test_migration_002.py",
      "content": "\"\"\"Tests for migration 002.\"\"\"\nimport pytest\nfrom sqlalchemy import create_engine, inspect, text\nfrom app.orchestrator_api.persistence.migrations.migration_002_add_token_tracking import upgrade, downgrade\n\nclass TestMigration002:\n    def test_upgrade_adds_columns(self, test_db_url):\n        upgrade(test_db_url)\n        \n        engine = create_engine(test_db_url)\n        inspector = inspect(engine)\n        columns = [col[\"name\"] for col in inspector.get_columns(\"pipeline_prompt_usage\")]\n        \n        assert \"input_tokens\" in columns\n        assert \"output_tokens\" in columns\n        assert \"cost_usd\" in columns\n        assert \"model\" in columns\n        assert \"execution_time_ms\" in columns\n    \n    def test_upgrade_idempotent(self, test_db_url):\n        upgrade(test_db_url)\n        upgrade(test_db_url)\n        \n        engine = create_engine(test_db_url)\n        inspector = inspect(engine)\n        columns = inspector.get_columns(\"pipeline_prompt_usage\")\n        assert len(columns) > 5\n    \n    def test_downgrade_removes_columns(self, test_db_url):\n        upgrade(test_db_url)\n        downgrade(test_db_url)\n        \n        engine = create_engine(test_db_url)\n        inspector = inspect(engine)\n        columns = [col[\"name\"] for col in inspector.get_columns(\"pipeline_prompt_usage\")]\n        \n        assert \"input_tokens\" not in columns\n        assert \"output_tokens\" not in columns"
    },
    {
      "path": "tests/unit/routers/test_health.py",
      "content": "\"\"\"Tests for health router.\"\"\"\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom app.orchestrator_api.main import app\n\nclient = TestClient(app)\n\nclass TestHealthRouter:\n    def test_health_returns_200(self):\n        response = client.get(\"/health\")\n        assert response.status_code == 200\n    \n    def test_health_returns_json(self):\n        response = client.get(\"/health\")\n        data = response.json()\n        assert \"status\" in data\n        assert data[\"status\"] == \"ok\""
    },
    {
      "path": "tests/integration/test_bootstrap_flow.py",
      "content": "\"\"\"Integration test for bootstrap flow.\"\"\"\nimport pytest\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nROOT = Path(__file__).parent.parent.parent\n\nclass TestBootstrapFlow:\n    def test_setup_script_executes(self):\n        python_path = sys.executable\n        setup_script = ROOT / \"scripts\" / \"setup.py\"\n        \n        result = subprocess.run(\n            [python_path, str(setup_script)],\n            capture_output=True,\n            timeout=300\n        )\n        \n        assert result.returncode == 0\n    \n    def test_server_starts(self):\n        import time\n        import requests\n        from threading import Thread\n        \n        python_path = sys.executable\n        run_script = ROOT / \"scripts\" / \"run.py\"\n        \n        def start_server():\n            subprocess.run([python_path, str(run_script)], timeout=5)\n        \n        server_thread = Thread(target=start_server, daemon=True)\n        server_thread.start()\n        time.sleep(2)\n        \n        try:\n            response = requests.get(\"http://localhost:8000/health\", timeout=1)\n            assert response.status_code == 200\n        except:\n            pytest.skip(\"Server did not start in time\")"
    }
  ],
  "dependencies": [
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "pydantic>=2.5.0",
    "sqlalchemy>=2.0.0",
    "python-dotenv>=1.0.0",
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "httpx>=0.25.0"
  ],
  "migration_scripts": [
    {
      "path": "setup.sh",
      "content": "#!/bin/bash\nset -e\n\necho \"Running setup for The Combine...\"\npython3 scripts/setup.py\nexit $?"
    },
    {
      "path": "setup.ps1",
      "content": "$ErrorActionPreference = \"Stop\"\n\nWrite-Host \"Running setup for The Combine...\"\npython scripts\\setup.py\nexit $LASTEXITCODE"
    },
    {
      "path": "run.sh",
      "content": "#!/bin/bash\nset -e\n\necho \"Starting The Combine server...\"\npython3 scripts/run.py\nexit $?"
    },
    {
      "path": "run.ps1",
      "content": "$ErrorActionPreference = \"Stop\"\n\nWrite-Host \"Starting The Combine server...\"\npython scripts\\run.py\nexit $LASTEXITCODE"
    }
  ]
}
