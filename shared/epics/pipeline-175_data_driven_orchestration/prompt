PM Mentor — Task: Create Epic
Epic ID: PIPELINE-175
Epic Title: Data-Driven Phase Orchestration & Prompt Management (Core Infrastructure)
Epic Summary

Transform pipeline execution into a fully data-driven, role-agnostic, and configuration-controlled system. Replace all hardcoded role/phase execution logic with:

A Phase Configuration model defining:

which role executes each phase

what artifact the role must produce

which phase follows next

phase-level configuration

A RolePrompt model defining:

role instruction blocks

versions of prompts

auditability of prompt usage

A generic PhaseExecutorService, capable of executing any phase using any role through a single function call.

This enables adding roles, phases, or entire pipeline shapes without any code changes — only configuration and prompt data.

Business Rationale

Enterprise customers require custom pipeline flows and role compositions

Adding roles (e.g., security_reviewer, ux_designer) must be configuration, not engineering

Reduces tech debt and complexity by eliminating per-role code paths

Sets the foundation for multi-tenant, customizable pipeline templates

Required for long-term evolution to autonomous agent workflows

Aligns with upcoming RepoProvider and LLMProvider abstractions

Revised & Corrected Story List (Final)
Story 0 — Define RolePrompt Model and Repository

Required models:

role_prompts table with fields:

id, role_name, version, starting_prompt, bootstrapper, instructions, working_schema

is_active, created_at, updated_at, created_by, notes

PipelinePromptUsage table:

id, pipeline_id, role_name, prompt_id, phase_name, used_at

Repository must support:

get_active_prompt(role_name)

get_by_id(prompt_id)

list_versions(role_name)

create(...)

Purpose: foundational for prompt management + auditability.

Story 1 — Create PhaseConfiguration Model and Repository

Defines the pipeline’s configuration-driven flow.

Fields:

id

phase_name

role_name

artifact_type

next_phase

is_active

config (JSON)

Repository methods:

get_by_phase(phase_name)

get_all_active()

create(...)

Adds the core state machine table.

Story 2 — Implement Generic PhaseExecutorService (with placeholder LLM integration)

Create PhaseExecutorService.execute_phase(pipeline_id, role, phase, expected_artifact_type)
Using:

RolePromptService

ArtifactService

PhaseConfigurationRepository

PipelineRepository

ArtifactRepository

LLM integration in MVP:

Direct Anthropic API call

Must be structured to allow future swap to LLMProvider (LLM-300)

Executor responsibilities:

Build prompt using RolePromptService

Execute LLM call

Extract JSON from response

Submit artifact via ArtifactService

Record prompt usage for audit

NOT included:

validation (handled by ArtifactService)

Story 3 — Robust LLM Response Parsing (No Duplicate Validation)

Implement safe JSON extraction for PhaseExecutor:

Capabilities:

Strip markdown fences

Handle surrounding text or chatter

Detect JSON substring and extract

Log parse failures clearly

Do not revalidate artifacts here.
Submit parsed JSON directly to ArtifactService for validation.

Example Test Cases
Test cases to handle:
1. Clean JSON: {"epic_id": "..."}
2. Markdown wrapped: ```json\n{...}\n```
3. Chatty response: "Here's the epic:\n{...}\nLet me know if..."
4. Multiple JSON blocks (take first/last? - specify)

Story 4 — Update PipelineService to Use Data-Driven Execution

Replace existing hardcoded logic with:

Fetch current phase configuration

Resolve next phase from PhaseConfiguration.next_phase

Update pipeline state

Record transition

Invoke PhaseExecutorService

No special-case code for PM/BA/Dev/QA.

Story 5 — Seed Scripts for Role Prompts & Phase Configuration

Create:

scripts/seed_role_prompts.py

Seeds default prompts for:

pm

architect

ba

dev

qa

commit (stub role — does nothing yet)

scripts/seed_phase_configuration.py

Seeds 6-phase pipeline:

pm → arch → ba → dev → qa → commit
Commit phase uses the stub role.

Both scripts must be:

idempotent

safe to run in deployments

Story 6 — Implement RolePromptService for Data-Driven Prompt Assembly

RolePromptService must:

Load prompt blocks from role_prompts

Inject:

pipeline context

artifact context

phase metadata

Return:

prompt text

prompt_id

No hardcoded prompts anywhere in orchestration.

Story 7 — Migration & Backward Compatibility Strategy

Before activation:

PIPELINE-150 tests must pass without changes

Backward compatibility maintained via:

feature flag (optional but preferred)

or migration helper

Deliverables:

Migration guide

Identification of deprecated code paths

Removal/deprecation timeline

Story 8 — Comprehensive Test Coverage

Required tests:

Unit Tests

PhaseConfigurationRepository

RolePromptRepository

RolePromptService

PhaseExecutorService JSON parsing

Integration Tests

Pipeline executes PM → Architect using data config

Adding a new role via data only (e.g., security_reviewer)

Executing new phase validates role/phase config

Regression Tests

All PIPELINE-150 tests remain green

API endpoints remain unchanged

Performance Test

Prompt build time <100ms

Removed or Deferred Items
❌ Removed from this Epic

Multi-artifact support (moved to PIPELINE-176)

LLMProvider abstraction (owned by LLM-300)

RepoProvider commit phase (owned by REPO-200)

Epic-Level Acceptance Criteria
AC-1 — Fully Data-Driven Pipeline Execution

Executing a phase uses:

execute_phase(pipeline_id, role, phase, artifact_type)


No role-specific functions exist.

AC-2 — Add New Role Without Code

Adding a role requires only:

new role_prompts row

new phase_configurations row

Pipeline must run the new phase successfully.

AC-3 — Prompt Auditability

Every phase execution stores:

pipeline_id

role_name

phase_name

prompt_id

AC-4 — Standard Pipeline Runs End-to-End

PM → Architect → BA → Dev → QA → Commit (stub)
must run using data-driven configuration only.

AC-5 — Backward Compatibility

All PIPELINE-150 tests pass

API behavior unchanged

Existing pipelines still execute properly

AC-6 — Documentation

Provide:

Guide: How to Add a New Role

Guide: How to Customize Pipeline Flow

Architecture diagram: data-driven pipeline execution