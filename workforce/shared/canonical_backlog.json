{
  "type": "canonical_backlog",
  "version": 1.1,
  "sections": {
    "summary": {
      "blocks": [
        {
          "id": "summary-overview",
          "type": "paragraph",
          "content": "The Canonical Backlog V1 defines all user stories for the Workbench AI MVP (Phase 1), workflow intelligence features (Phase 2), and growth capabilities (Phase 3). All stories are INVEST-compliant with detailed acceptance criteria, story point estimates (Fibonacci scale), phase assignments, dependency tracking, and technical implementation notes. The backlog has been reconciled with Canonical Architecture V1.1 to ensure all file paths, import patterns, and repository references align with the fixed structure: experience/ as project root, app/ as canonical Python package."
        },
        {
          "id": "summary-phases",
          "type": "paragraph",
          "content": "Phase 1 (MVP) delivers 22 stories totaling 117 story points across 3 months. Phase 2 (Workflow Intelligence) adds 7 stories totaling 52 story points over 2-3 months. Phase 3 (Growth & Expansion) includes 3 stories totaling 29 story points over 2-3 months. Total backlog: 32 stories, 198 story points, estimated 7-9 months with 2-3 engineers at ~39 story points per month velocity."
        },
        {
          "id": "summary-themes",
          "type": "paragraph",
          "content": "Stories are organized by theme and Epic feature: Authentication (magic link Phase 1, third-party Phase 2), Workspace Management (CRUD operations), PM Discovery (questions and Epic generation), Architecture Generation (multi-agent with Mentor consolidation), Backlog Generation (BA agents with INVEST compliance), Export (Markdown and JSON with Jira compatibility), Real-Time Streaming (SSE for agent progress), Error Handling (retry logic and validation), Pipeline Re-runs (regenerate stages), and Repository Introspection (read-only file access for AI agents)."
        }
      ]
    },
    "phase_1_stories": {
      "blocks": [
        {
          "id": "phase1-auth-heading",
          "type": "heading",
          "content": "Authentication",
          "level": 2
        },
        {
          "id": "story-auth-101",
          "type": "story",
          "data": {
            "id": "AUTH-101",
            "title": "Magic link authentication",
            "user_story": "As a user, I want to authenticate using a magic link sent to my email so that I can access my workspaces without managing passwords.",
            "acceptance_criteria": [
              "User enters email address in login form",
              "System generates one-time token (UUID) and stores hash in sessions table with 7-day expiration",
              "System sends magic link email via SMTP (stdlib smtplib) or configurable HTTP POST endpoint",
              "User clicks magic link: GET /magic-login?token=...",
              "System validates token, creates session record, sets secure HttpOnly cookie with session ID",
              "Session cookie persists for 7 days or until logout",
              "FastAPI dependency validates session cookie on protected routes",
              "Logout endpoint deletes session record and clears cookie",
              "Invalid/expired tokens return clear error message with re-send option"
            ],
            "story_points": 5,
            "phase": "Phase 1",
            "epic_feature": "Authentication",
            "dependencies": [],
            "technical_notes": "Implementation locations: app/routers/auth.py (login, magic-login, logout endpoints), app/services/email_service.py (SMTP or HTTP POST for magic link delivery), app/models/sessions.py (SQLAlchemy model for sessions table). Sessions table schema: id (UUID), email (string), token_hash (string, bcrypt), expires_at (ISO timestamp, 7 days from creation), created_at (ISO timestamp). Use secrets.token_urlsafe(32) for token generation. FastAPI dependency: get_current_user reads session cookie, validates against sessions table, returns user email or raises 401. Magic link format: https://app.workbench.ai/magic-login?token=<token>. Email template location: templates/email/magic_link.html (Jinja2). SMTP config via environment variables: SMTP_HOST, SMTP_PORT, SMTP_USER, SMTP_PASSWORD. Alternative: HTTP POST to configurable webhook URL for email delivery services."
          }
        },
        {
          "id": "phase1-workspace-heading",
          "type": "heading",
          "content": "Workspace Management",
          "level": 2
        },
        {
          "id": "story-wrk-100",
          "type": "story",
          "data": {
            "id": "WRK-100",
            "title": "Create workspace",
            "user_story": "As a user, I want to create a new workspace with a name and description so that I can organize my product ideas.",
            "acceptance_criteria": [
              "User provides workspace name (required, max 100 chars) and description (optional, max 500 chars)",
              "System creates workspace record with status='active'",
              "System creates initial pipeline_state record with stage='not_started'",
              "System redirects to workspace detail view",
              "Workspace appears in user's workspace list",
              "User can only see their own workspaces (row-level security via user_id)"
            ],
            "story_points": 2,
            "phase": "Phase 1",
            "epic_feature": "Workspace Management",
            "dependencies": ["AUTH-101"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (POST /workspaces endpoint), app/models/workspace.py (Workspace SQLAlchemy model), app/models/pipeline_state.py (PipelineState SQLAlchemy model), app/schemas/workspace.py (WorkspaceCreate, WorkspaceResponse Pydantic models). Template: templates/workspace/create.html (HTMX form). Form validation: Pydantic model with min/max length constraints. Database transaction: Create workspace and pipeline_state in single transaction. Row-level security: Filter by user_id from session in all queries. UUID generation: Use uuid.uuid4() for workspace.id. Timestamps: ISO 8601 format via datetime.now(timezone.utc).isoformat()."
          }
        },
        {
          "id": "story-wrk-101",
          "type": "story",
          "data": {
            "id": "WRK-101",
            "title": "View workspace list",
            "user_story": "As a user, I want to view a list of all my workspaces so that I can select one to work on.",
            "acceptance_criteria": [
              "User sees all their active workspaces sorted by updated_at DESC",
              "Each workspace shows: name, description (truncated to 100 chars), created_at, updated_at",
              "List includes visual indicator of pipeline stage (e.g., 'PM Questions', 'Epic Approval', 'Complete')",
              "Clicking workspace navigates to detail view",
              "Empty state shows 'Create your first workspace' message with CTA button",
              "Archived workspaces hidden by default (toggleable filter)"
            ],
            "story_points": 2,
            "phase": "Phase 1",
            "epic_feature": "Workspace Management",
            "dependencies": ["AUTH-101", "WRK-100"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (GET /workspaces endpoint), templates/workspace/list.html (Jinja2 template with HTMX). Query: db.query(Workspace).filter(Workspace.user_id == current_user.id, Workspace.status == 'active').order_by(Workspace.updated_at.desc()).all(). Join with pipeline_state to show current stage. Template shows workspace cards with HTMX hx-get to load detail view. Empty state handled via Jinja2 conditional. Archive filter: HTMX checkbox toggles between status='active' and status IN ('active', 'archived')."
          }
        },
        {
          "id": "story-wrk-102",
          "type": "story",
          "data": {
            "id": "WRK-102",
            "title": "Archive workspace",
            "user_story": "As a user, I want to archive a workspace so that I can hide completed or abandoned ideas without deleting them.",
            "acceptance_criteria": [
              "User clicks 'Archive' button on workspace detail or list view",
              "System updates workspace.status to 'archived'",
              "Workspace disappears from default list view",
              "Archived workspaces appear in 'Archived' filter view",
              "User can unarchive workspace to restore to active status",
              "Archived workspaces retain all data (canonical documents, pipeline state, agent logs)"
            ],
            "story_points": 1,
            "phase": "Phase 1",
            "epic_feature": "Workspace Management",
            "dependencies": ["AUTH-101", "WRK-100", "WRK-101"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (PATCH /workspaces/{id}/archive and PATCH /workspaces/{id}/unarchive endpoints). Update query: db.query(Workspace).filter(Workspace.id == workspace_id, Workspace.user_id == current_user.id).update({\"status\": \"archived\", \"updated_at\": now_iso()}). HTMX: Archive button triggers hx-patch with hx-swap to remove row from list. Unarchive: Same pattern but sets status='active'. No data deletion (soft delete pattern)."
          }
        },
        {
          "id": "story-wrk-103",
          "type": "story",
          "data": {
            "id": "WRK-103",
            "title": "Delete workspace",
            "user_story": "As a user, I want to permanently delete a workspace so that I can remove unwanted ideas and free up storage.",
            "acceptance_criteria": [
              "User clicks 'Delete' button on workspace detail or list view",
              "System shows confirmation modal: 'This action cannot be undone. Delete workspace?'",
              "On confirmation, system sets workspace.status to 'deleted' (soft delete)",
              "System also soft-deletes related records: canonical_documents, pipeline_state (set status='deleted')",
              "Deleted workspaces never appear in any user-facing views",
              "Admin can view deleted workspaces for audit purposes (future story)"
            ],
            "story_points": 2,
            "phase": "Phase 1",
            "epic_feature": "Workspace Management",
            "dependencies": ["AUTH-101", "WRK-100"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (DELETE /workspaces/{id} endpoint), templates/workspace/delete_modal.html (confirmation modal). Soft delete: Update workspace.status='deleted', canonical_documents.status='deleted' for all related docs, pipeline_state.stage='deleted'. HTMX: Delete button shows modal via hx-get, modal confirm button triggers hx-delete with hx-swap='outerHTML' to remove from DOM. Transaction: Wrap all updates in single database transaction. No hard deletes (preserve audit trail)."
          }
        },
        {
          "id": "phase1-pm-heading",
          "type": "heading",
          "content": "PM Discovery",
          "level": 2
        },
        {
          "id": "story-pm-100",
          "type": "story",
          "data": {
            "id": "PM-100",
            "title": "Generate PM discovery questions",
            "user_story": "As a user, I want the system to generate clarifying questions about my product idea so that I can provide complete information for Epic generation.",
            "acceptance_criteria": [
              "User clicks 'Start PM Flow' on workspace detail page",
              "System spawns 3 PM agents in parallel via asyncio.gather",
              "Each PM agent proposes 8-12 clarifying questions based on workspace name and description",
              "PM Mentor agent consolidates 3 proposals into single set of 8-12 final questions",
              "System stores questions in canonical_documents table (doc_type='pm_questions', status='draft')",
              "Questions validate against PMQuestionsV1 Pydantic schema before storage",
              "User sees questions rendered in form with text inputs (one per question)",
              "SSE stream shows real-time progress: 'PM Agent 1 working...', 'PM Mentor consolidating...'"
            ],
            "story_points": 5,
            "phase": "Phase 1",
            "epic_feature": "PM Discovery",
            "dependencies": ["AUTH-101", "WRK-100"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (POST /workspaces/{id}/start-pm endpoint), app/services/orchestrator_core.py (pm_discovery_stage function), app/services/pm_agent_service.py (PMAgentService.generate_questions method), app/schemas/pm_questions.py (PMQuestionsV1 Pydantic model). Orchestrator spawns agents via: questions_lists = await asyncio.gather(pm1.generate(), pm2.generate(), pm3.generate()). PM Mentor consolidates via: final_questions = await pm_mentor.consolidate(questions_lists). Schema validation: PMQuestionsV1.model_validate(final_questions) before INSERT. Agent logs: Store all 4 agent calls (3 PM + 1 Mentor) in agent_logs table. SSE: Yield events during orchestrator execution. Template: templates/pm/questions_form.html."
          }
        },
        {
          "id": "story-pm-101",
          "type": "story",
          "data": {
            "id": "PM-101",
            "title": "Submit PM answers and generate Epic",
            "user_story": "As a user, I want to answer PM questions and receive a Canonical Epic so that I have a validated product definition.",
            "acceptance_criteria": [
              "User fills in all PM question answers (min 10 chars per answer, validation via Pydantic)",
              "User clicks 'Submit Answers' button",
              "System stores answers in pipeline_state.context (JSON field)",
              "System spawns 3 PM agents to generate Epic proposals (based on questions + answers)",
              "PM Mentor consolidates 3 Epic proposals into single CanonicalEpicV1",
              "System validates Epic against CanonicalEpicV1 Pydantic schema",
              "If validation fails, PM Mentor rewrites (max 3 attempts)",
              "On success, system stores Epic in canonical_documents (doc_type='epic', status='draft')",
              "System transitions pipeline_state.stage to 'epic_approval'",
              "User sees Epic rendered with approval UI (Approve/Reject buttons)"
            ],
            "story_points": 8,
            "phase": "Phase 1",
            "epic_feature": "PM Discovery",
            "dependencies": ["AUTH-101", "WRK-100", "PM-100"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (POST /workspaces/{id}/submit-answers endpoint), app/services/orchestrator_core.py (epic_generation_stage function), app/services/pm_agent_service.py (PMAgentService.generate_epic method). Validation: Pydantic model for answers (list of strings, min_length=10 each). Context storage: JSON.dumps(answers) → pipeline_state.context. Epic generation: 3 parallel agents via asyncio.gather, PM Mentor consolidation. Schema validation loop: for attempt in range(3): try: CanonicalEpicV1.model_validate(epic); break; except ValidationError as e: epic = await mentor.rewrite(epic, e.errors()). Agent logs: Store all attempts. Template: templates/epic/approval.html (render Epic sections with Jinja2)."
          }
        },
        {
          "id": "phase1-epic-heading",
          "type": "heading",
          "content": "Epic Generation",
          "level": 2
        },
        {
          "id": "story-epic-100",
          "type": "story",
          "data": {
            "id": "EPIC-100",
            "title": "Approve or reject Epic",
            "user_story": "As a user, I want to approve or reject the generated Epic so that I control whether the pipeline proceeds to Architecture generation.",
            "acceptance_criteria": [
              "User reviews Epic sections: Vision, Problem, Goals, In Scope, Out of Scope, Requirements, Risks, Roadmap",
              "User can inline-edit any Epic section (HTMX form with PATCH endpoint)",
              "User clicks 'Approve Epic' button",
              "System updates canonical_documents.status='approved' for Epic",
              "System transitions pipeline_state.stage to 'architecture_start'",
              "User clicks 'Reject Epic' button (alternative flow)",
              "System marks Epic as status='superseded' and re-runs PM-101 (regenerate Epic)",
              "Approved Epic becomes immutable (no further edits allowed without regeneration)"
            ],
            "story_points": 3,
            "phase": "Phase 1",
            "epic_feature": "Epic Generation",
            "dependencies": ["AUTH-101", "WRK-100", "PM-101"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (POST /workspaces/{id}/approve-epic and POST /workspaces/{id}/reject-epic endpoints), app/routers/epic.py (PATCH /workspaces/{id}/epic endpoint for inline edits). Approve: Update canonical_documents SET status='approved' WHERE workspace_id=... AND doc_type='epic'. Update pipeline_state SET stage='architecture_start'. Reject: Update canonical_documents SET status='superseded', trigger orchestrator_core.epic_generation_stage() again. Inline edit: HTMX hx-patch on section, validate change against CanonicalEpicV1 schema, update canonical_documents.content (JSON column). Template: templates/epic/approval.html with inline edit forms."
          }
        },
        {
          "id": "story-epic-101",
          "type": "story",
          "data": {
            "id": "EPIC-101",
            "title": "Re-run Epic generation",
            "user_story": "As a user, I want to regenerate the Epic if I'm not satisfied with the output so that I can get a better result without starting over.",
            "acceptance_criteria": [
              "User clicks 'Regenerate Epic' button on Epic approval page",
              "System marks current Epic as status='superseded'",
              "System re-runs PM-101 flow: 3 PM agents + PM Mentor generate new Epic",
              "New Epic stored as status='draft'",
              "User sees new Epic with approval UI",
              "Pipeline stage remains 'epic_approval'",
              "All previous Epic versions preserved in canonical_documents (status='superseded')",
              "SSE stream shows regeneration progress"
            ],
            "story_points": 5,
            "phase": "Phase 1",
            "epic_feature": "Epic Generation",
            "dependencies": ["AUTH-101", "WRK-100", "PM-101", "EPIC-100"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (POST /workspaces/{id}/regenerate-epic endpoint), app/services/orchestrator_core.py (reuse epic_generation_stage function). Supersede current: UPDATE canonical_documents SET status='superseded' WHERE workspace_id=... AND doc_type='epic' AND status='draft'. Re-run: Call orchestrator_core.epic_generation_stage(workspace_id) which spawns 3 PM agents + Mentor. SSE: Stream progress events. Version history: All superseded Epics remain in database (support for version history UI in Phase 2)."
          }
        },
        {
          "id": "phase1-arch-heading",
          "type": "heading",
          "content": "Architecture Generation",
          "level": 2
        },
        {
          "id": "story-arch-100",
          "type": "story",
          "data": {
            "id": "ARCH-100",
            "title": "Generate Architecture",
            "user_story": "As a user, I want the system to generate a technical architecture based on my approved Epic so that I have implementation guidance.",
            "acceptance_criteria": [
              "After Epic approval, system automatically triggers Architecture generation",
              "System spawns 3 Architect agents in parallel (each receives approved Epic as context)",
              "Each Architect agent proposes CanonicalArchitectureV1 (with sections: summary, diagram, components, data_model, key_decisions, etc.)",
              "Architect Mentor consolidates 3 proposals into single CanonicalArchitectureV1",
              "System validates Architecture against CanonicalArchitectureV1 Pydantic schema",
              "If validation fails, Architect Mentor rewrites (max 3 attempts)",
              "On success, system stores Architecture in canonical_documents (doc_type='architecture', status='draft')",
              "System transitions pipeline_state.stage to 'architecture_approval'",
              "SSE stream shows progress: 'Architect 1 working...', 'Architect Mentor consolidating...'"
            ],
            "story_points": 8,
            "phase": "Phase 1",
            "epic_feature": "Architecture Generation",
            "dependencies": ["AUTH-101", "WRK-100", "EPIC-100"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (POST /workspaces/{id}/start-architecture endpoint, auto-triggered after Epic approval), app/services/orchestrator_core.py (architecture_generation_stage function), app/services/architecture_agent_service.py (ArchitectureAgentService.generate_architecture method), app/schemas/canonical_architecture.py (CanonicalArchitectureV1 Pydantic model). Context: Read approved Epic from canonical_documents WHERE doc_type='epic' AND status='approved'. Parallel agents: architectures = await asyncio.gather(arch1.generate(epic), arch2.generate(epic), arch3.generate(epic)). Mentor consolidation: final_arch = await mentor.consolidate(architectures). Schema validation loop: Same 3-attempt pattern as Epic. Agent logs: Store all attempts. Template: templates/architecture/approval.html."
          }
        },
        {
          "id": "story-arch-101",
          "type": "story",
          "data": {
            "id": "ARCH-101",
            "title": "Approve or reject Architecture",
            "user_story": "As a user, I want to approve or reject the generated Architecture so that I control whether the pipeline proceeds to Backlog generation.",
            "acceptance_criteria": [
              "User reviews Architecture sections: Summary, Diagram, Components, Data Model, Key Decisions, etc.",
              "User can ask ad-hoc questions about Architecture (e.g., 'Why SQLite instead of PostgreSQL?')",
              "System answers via single Architect Mentor call, displays answer inline (no Architecture modification)",
              "User can inline-edit any Architecture section (HTMX form with PATCH endpoint)",
              "User clicks 'Approve Architecture' button",
              "System updates canonical_documents.status='approved' for Architecture",
              "System transitions pipeline_state.stage to 'ba_start'",
              "User clicks 'Reject Architecture' or 'Regenerate' button (alternative flow)",
              "System marks Architecture as status='superseded' and re-runs ARCH-100"
            ],
            "story_points": 5,
            "phase": "Phase 1",
            "epic_feature": "Architecture Generation",
            "dependencies": ["AUTH-101", "WRK-100", "ARCH-100"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (POST /workspaces/{id}/approve-architecture, POST /workspaces/{id}/reject-architecture endpoints), app/routers/architecture.py (POST /workspaces/{id}/architecture/ask endpoint for ad-hoc Q&A, PATCH /workspaces/{id}/architecture for inline edits). Ad-hoc Q&A: Single Architect Mentor call with Architecture + question as context, return answer as JSON {\"question\": \"...\", \"answer\": \"...\"}. SSE: Stream answer tokens. Inline edit: Validate against CanonicalArchitectureV1 schema. Approve/Reject: Same pattern as Epic. Template: templates/architecture/approval.html with Q&A form."
          }
        },
        {
          "id": "phase1-backlog-heading",
          "type": "heading",
          "content": "Backlog Generation",
          "level": 2
        },
        {
          "id": "story-backlog-100",
          "type": "story",
          "data": {
            "id": "BACKLOG-100",
            "title": "Generate Backlog",
            "user_story": "As a user, I want the system to generate a prioritized backlog of user stories based on my approved Epic and Architecture so that I have actionable development tasks.",
            "acceptance_criteria": [
              "After Architecture approval, system automatically triggers Backlog generation",
              "System spawns 3 BA agents in parallel (each receives Epic + Architecture as context)",
              "Each BA agent proposes 20-40 INVEST-compliant user stories with acceptance criteria, story points (Fibonacci), phase assignment, dependencies",
              "BA Mentor consolidates 3 proposals into single CanonicalBacklogV1 with ~20-30 stories for Phase 1",
              "System validates Backlog against CanonicalBacklogV1 Pydantic schema",
              "If validation fails, BA Mentor rewrites (max 3 attempts)",
              "On success, system stores Backlog in canonical_documents (doc_type='backlog', status='draft')",
              "System transitions pipeline_state.stage to 'backlog_approval'",
              "SSE stream shows progress: 'BA Agent 1 working...', 'BA Mentor consolidating...'"
            ],
            "story_points": 8,
            "phase": "Phase 1",
            "epic_feature": "Backlog Generation",
            "dependencies": ["AUTH-101", "WRK-100", "ARCH-101"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (POST /workspaces/{id}/start-backlog endpoint, auto-triggered after Architecture approval), app/services/orchestrator_core.py (backlog_generation_stage function), app/services/backlog_agent_service.py (BacklogAgentService.generate_backlog method), app/schemas/canonical_backlog.py (CanonicalBacklogV1 Pydantic model). Context: Read approved Epic and Architecture from canonical_documents. Parallel agents: backlogs = await asyncio.gather(ba1.generate(epic, arch), ba2.generate(epic, arch), ba3.generate(epic, arch)). Mentor consolidation: final_backlog = await mentor.consolidate(backlogs). Schema validation: CanonicalBacklogV1.model_validate(backlog) with 3-attempt loop. Agent logs: Store all attempts. Template: templates/backlog/approval.html."
          }
        },
        {
          "id": "story-backlog-101",
          "type": "story",
          "data": {
            "id": "BACKLOG-101",
            "title": "Approve or reject Backlog",
            "user_story": "As a user, I want to approve or reject the generated Backlog so that I can finalize my product definition and export it.",
            "acceptance_criteria": [
              "User reviews Backlog: Phase 1/2/3 stories with acceptance criteria, story points, dependencies",
              "User can inline-edit any story field (title, user_story, acceptance_criteria, story_points, etc.)",
              "User clicks 'Approve Backlog' button",
              "System updates canonical_documents.status='approved' for Backlog",
              "System transitions pipeline_state.stage to 'complete'",
              "User sees 'Export' buttons (Markdown, JSON)",
              "User clicks 'Reject Backlog' or 'Regenerate' button (alternative flow)",
              "System marks Backlog as status='superseded' and re-runs BACKLOG-100"
            ],
            "story_points": 5,
            "phase": "Phase 1",
            "epic_feature": "Backlog Generation",
            "dependencies": ["AUTH-101", "WRK-100", "BACKLOG-100"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (POST /workspaces/{id}/approve-backlog, POST /workspaces/{id}/reject-backlog endpoints), app/routers/backlog.py (PATCH /workspaces/{id}/backlog endpoint for inline edits). Inline edit: Validate against CanonicalBacklogV1 schema, update canonical_documents.content (JSON column). Approve: Update status='approved', set stage='complete'. Reject: Update status='superseded', re-run backlog_generation_stage(). Template: templates/backlog/approval.html with inline edit forms."
          }
        },
        {
          "id": "phase1-export-heading",
          "type": "heading",
          "content": "Export",
          "level": 2
        },
        {
          "id": "story-export-100",
          "type": "story",
          "data": {
            "id": "EXPORT-100",
            "title": "Markdown export",
            "user_story": "As a user, I want to export my approved Epic, Architecture, and Backlog as a single Markdown file so that I can share it with my team or store it externally.",
            "acceptance_criteria": [
              "User clicks 'Export as Markdown' button on workspace detail page (only visible when pipeline_state.stage='complete')",
              "System reads approved Epic, Architecture, Backlog from canonical_documents (status='approved')",
              "System renders Markdown file using Jinja2 template at templates/export/markdown_export.jinja2",
              "Markdown includes: Table of Contents, Epic (all sections), Architecture (all sections), Backlog (all stories with acceptance criteria)",
              "System saves temporary file to experience/tmp/exports/{workspace_id}_{timestamp}.md",
              "System returns FileResponse with Content-Disposition: attachment; filename='{workspace_name}.md'",
              "Temporary file deleted after 1 hour or on next export request",
              "Returns 404 if no approved documents exist"
            ],
            "story_points": 3,
            "phase": "Phase 1",
            "epic_feature": "Export",
            "dependencies": ["AUTH-101", "WRK-100", "EPIC-100", "ARCH-101", "BACKLOG-101"],
            "technical_notes": "Implementation locations: app/routers/export.py (GET /workspaces/{id}/export?format=markdown endpoint), app/services/export_service.py (ExportService.generate_markdown method), templates/export/markdown_export.jinja2 (Jinja2 template). Query: SELECT content FROM canonical_documents WHERE workspace_id=... AND doc_type IN ('epic', 'architecture', 'backlog') AND status='approved'. Template rendering: template.render(epic=epic_content, architecture=arch_content, backlog=backlog_content). File creation: Path('experience/tmp/exports').mkdir(exist_ok=True); filepath = f'experience/tmp/exports/{workspace_id}_{int(time.time())}.md'. FileResponse: return FileResponse(filepath, media_type='text/markdown', filename=f'{workspace.name}.md'). Cleanup: Background task or cron deletes files older than 1 hour."
          }
        },
        {
          "id": "story-export-101",
          "type": "story",
          "data": {
            "id": "EXPORT-101",
            "title": "JSON export with Jira compatibility",
            "user_story": "As a user, I want to export my approved Epic, Architecture, and Backlog as JSON with Jira-compatible structure so that I can import it into Jira or other project management tools.",
            "acceptance_criteria": [
              "User clicks 'Export as JSON' button on workspace detail page",
              "System reads approved Epic, Architecture, Backlog from canonical_documents",
              "System generates JSON with structure: {workspace: {...}, epic: {...}, architecture: {...}, backlog: {...}, jira_metadata: {...}}",
              "jira_metadata includes: import_format version, epic_key (null for new), stories array with Jira-compatible fields (summary, description, acceptance_criteria, story_points, epic_link, dependencies)",
              "System maps CanonicalBacklogV1 stories to Jira issue format with Epic → Feature → Story hierarchy",
              "System saves temporary file to experience/tmp/exports/{workspace_id}_{timestamp}.json",
              "System returns FileResponse with Content-Disposition: attachment; filename='{workspace_name}.json'",
              "Temporary file deleted after 1 hour"
            ],
            "story_points": 5,
            "phase": "Phase 1",
            "epic_feature": "Export",
            "dependencies": ["AUTH-101", "WRK-100", "EXPORT-100"],
            "technical_notes": "Implementation locations: app/routers/export.py (GET /workspaces/{id}/export?format=json endpoint), app/services/export_service.py (ExportService.generate_json method), app/schemas/export.py (JiraExportSchema Pydantic model for validation). Jira mapping: Epic phases → Jira epics, stories → Jira issues with fields: {\"summary\": story.title, \"description\": story.user_story, \"customfield_10000\": story.acceptance_criteria (array), \"story_points\": story.story_points, \"epic_link\": epic_key, \"depends_on\": story.dependencies}. JSON structure: {\"workspace\": {\"id\": ..., \"name\": ...}, \"epic\": CanonicalEpicV1, \"architecture\": CanonicalArchitectureV1, \"backlog\": CanonicalBacklogV1, \"jira_metadata\": {\"import_format\": \"1.0\", \"epic_key\": null, \"stories\": [...]}}. FileResponse: media_type='application/json'."
          }
        },
        {
          "id": "phase1-sse-heading",
          "type": "heading",
          "content": "Real-Time Streaming",
          "level": 2
        },
        {
          "id": "story-sse-100",
          "type": "story",
          "data": {
            "id": "SSE-100",
            "title": "Real-time agent progress streaming",
            "user_story": "As a user, I want to see real-time progress updates while agents generate Epic, Architecture, and Backlog so that I know the system is working and how long it will take.",
            "acceptance_criteria": [
              "User sees live progress updates during PM, Architect, and BA agent execution",
              "SSE endpoint: GET /workspaces/{workspace_id}/stream returns text/event-stream",
              "Event types: agent_start (agent name, stage), agent_progress (message), agent_complete (output preview), stage_complete (stage name)",
              "Events properly formatted: data: {\"type\": \"agent_start\", \"agent\": \"PM Agent 1\", \"stage\": \"pm_questions\"}\\n\\n",
              "Frontend uses EventSource API to consume stream and update UI progressively",
              "Progress indicators: Spinner for active agents, checkmark for completed agents, progress bar for overall stage",
              "Stream closes when stage completes (sends stage_complete event then closes connection)",
              "EventSource automatically reconnects if connection drops mid-stream"
            ],
            "story_points": 8,
            "phase": "Phase 1",
            "epic_feature": "Real-Time Streaming",
            "dependencies": ["AUTH-101", "WRK-100", "PM-100"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (GET /workspaces/{id}/stream endpoint), app/services/orchestrator_core.py (async generator yields SSE events during agent execution), templates/workspace/detail.html (EventSource JavaScript). SSE endpoint: async def stream_events(): async for event in orchestrator_core.execute_stage(workspace_id, stage): yield f'data: {json.dumps(event)}\\n\\n'. FastAPI: return StreamingResponse(stream_events(), media_type='text/event-stream'). Orchestrator yields: yield {\"type\": \"agent_start\", \"agent\": \"PM Agent 1\", \"stage\": stage} before each agent call, yield {\"type\": \"agent_complete\", \"agent\": \"PM Agent 1\"} after completion. Frontend: const eventSource = new EventSource('/workspaces/{id}/stream'); eventSource.onmessage = (e) => { const event = JSON.parse(e.data); updateProgressUI(event); }. Close: eventSource.close() when stage_complete received."
          }
        },
        {
          "id": "phase1-error-heading",
          "type": "heading",
          "content": "Error Handling",
          "level": 2
        },
        {
          "id": "story-error-100",
          "type": "story",
          "data": {
            "id": "ERROR-100",
            "title": "Comprehensive error handling",
            "user_story": "As a user, I want clear error messages and automatic retry logic so that transient failures don't block my workflow.",
            "acceptance_criteria": [
              "Pydantic validation errors (422): Return field-level error details for invalid requests",
              "Database errors (500): Return generic error message, log full error details",
              "LLM API errors: Implement retry logic (2 attempts, exponential backoff: 1s, 2s), log all attempts to agent_logs",
              "Schema validation errors: Trigger Mentor rewrite loop (max 3 attempts), show validation errors to user if all attempts fail",
              "Authentication errors (401): Return when session invalid or missing",
              "Authorization errors (403): Return when user attempts to access another user's workspace",
              "Not found errors (404): Return for non-existent workspaces/documents",
              "Rate limit errors (429): Return when user exceeds 60 requests/minute",
              "All errors logged with context (user_id, workspace_id, endpoint, timestamp)",
              "User-friendly error messages with actionable guidance (e.g., 'Retry', 'Contact support')"
            ],
            "story_points": 5,
            "phase": "Phase 1",
            "epic_feature": "Error Handling",
            "dependencies": ["AUTH-101"],
            "technical_notes": "Implementation locations: app/main.py (global exception handlers using @app.exception_handler), app/services/pm_agent_service.py and other agent services (retry logic), app/schemas/ (Pydantic validators). Global handlers: @app.exception_handler(SQLAlchemyError) async def db_error_handler(request, exc): log.error(f'DB error: {exc}'); return JSONResponse(status_code=500, content={'detail': 'Database error occurred'}). Retry logic: for attempt in range(2): try: response = await anthropic_client.messages.create(...); break; except APIError as e: if attempt == 1: raise; await asyncio.sleep(2 ** attempt). Schema validation: Same 3-attempt loop in orchestrator_core. Rate limiting: FastAPI middleware with slowapi or custom middleware counting requests per session. Agent logs: INSERT INTO agent_logs (error=str(exc)) on failure."
          }
        },
        {
          "id": "story-error-101",
          "type": "story",
          "data": {
            "id": "ERROR-101",
            "title": "Error recovery UI",
            "user_story": "As a user, I want to see helpful error messages with retry buttons so that I can recover from failures without losing my progress.",
            "acceptance_criteria": [
              "LLM API failures show: 'Generation failed. This is usually temporary. [Retry]' button",
              "Schema validation failures show: 'Output validation failed after 3 attempts. [Regenerate] or [Contact Support]'",
              "Database errors show: 'Could not save data. Please try again. [Retry]'",
              "Network errors show: 'Connection lost. [Retry]' with auto-retry after 5 seconds",
              "Retry button calls same endpoint again (idempotent operations)",
              "Error messages displayed inline (not full-page errors, use HTMX hx-swap for targeted updates)",
              "Errors cleared automatically on successful retry",
              "Agent logs preserve all failure details for debugging"
            ],
            "story_points": 3,
            "phase": "Phase 1",
            "epic_feature": "Error Handling",
            "dependencies": ["AUTH-101", "ERROR-100"],
            "technical_notes": "Implementation locations: templates/components/error_message.html (reusable error component), app/routers/ (endpoints return error responses with HTMX headers). Error component: <div class='error-message'><p>{error_detail}</p><button hx-post='{retry_endpoint}' hx-swap='outerHTML'>Retry</button></div>. HTMX error handling: htmx.on('htmx:responseError', function(evt) { showErrorMessage(evt.detail.xhr.responseText); }). Auto-retry: setTimeout(() => htmx.trigger('#retry-btn', 'click'), 5000). Idempotent operations: Use PUT/PATCH for updates, check pipeline_state before executing stage to prevent duplicate runs."
          }
        },
        {
          "id": "phase1-rerun-heading",
          "type": "heading",
          "content": "Pipeline Re-runs",
          "level": 2
        },
        {
          "id": "story-rerun-100",
          "type": "story",
          "data": {
            "id": "RERUN-100",
            "title": "Re-run any pipeline stage",
            "user_story": "As a user, I want to re-run Epic, Architecture, or Backlog generation at any time so that I can iterate on my product definition without starting from scratch.",
            "acceptance_criteria": [
              "User clicks 'Regenerate Epic' button (visible on Epic approval page)",
              "System marks current Epic as status='superseded', re-runs PM-101 (Epic generation)",
              "User clicks 'Regenerate Architecture' button (visible on Architecture approval page)",
              "System marks current Architecture as status='superseded', re-runs ARCH-100 (reads latest approved Epic)",
              "User clicks 'Regenerate Backlog' button (visible on Backlog approval page)",
              "System marks current Backlog as status='superseded', re-runs BACKLOG-100 (reads latest approved Epic + Architecture)",
              "All previous versions preserved in canonical_documents (status='superseded')",
              "Pipeline stage resets to appropriate approval stage (e.g., epic_approval, architecture_approval)",
              "SSE stream shows regeneration progress"
            ],
            "story_points": 8,
            "phase": "Phase 1",
            "epic_feature": "Pipeline Re-runs",
            "dependencies": ["AUTH-101", "WRK-100", "EPIC-101", "ARCH-101", "BACKLOG-101"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (POST /workspaces/{id}/regenerate-epic, POST /workspaces/{id}/regenerate-architecture, POST /workspaces/{id}/regenerate-backlog endpoints), app/services/orchestrator_core.py (reuse stage functions: epic_generation_stage, architecture_generation_stage, backlog_generation_stage). Regenerate Epic: UPDATE canonical_documents SET status='superseded' WHERE workspace_id=... AND doc_type='epic' AND status IN ('draft', 'approved'); await orchestrator_core.epic_generation_stage(workspace_id); UPDATE pipeline_state SET stage='epic_approval'. Regenerate Architecture: Same pattern, reads latest approved Epic via: SELECT content FROM canonical_documents WHERE workspace_id=... AND doc_type='epic' AND status='approved' ORDER BY created_at DESC LIMIT 1. Regenerate Backlog: Reads latest approved Epic + Architecture. SSE: Stream progress via StreamingResponse. Version history: All superseded versions remain in database."
          }
        },
        {
          "id": "phase1-repo-heading",
          "type": "heading",
          "content": "Repository Introspection (Read-Only)",
          "level": 2
        },
        {
          "id": "story-repo-100",
          "type": "story",
          "data": {
            "id": "REPO-100",
            "title": "List repo files (read-only)",
            "user_story": "As an AI Dev Orchestrator, I want to list files in the repository so that I can discover the project structure before making code changes.",
            "acceptance_criteria": [
              "Endpoint GET /repo/files accepts query params: root (required), glob (optional), max_files (optional, default 200)",
              "Allow-list enforcement: Only permits roots: app, templates, tests, static, pyproject.toml, README.md",
              "Forbidden path blocking: Returns 403 for .env, .git, __pycache__, secrets, or any path outside allow-list",
              "Binary file exclusion: Filters out .pyc, .db, .sqlite, images, etc.",
              "Response format: { root: 'app', files: ['app/routers/repo_view.py', ...], truncated: false }",
              "Path traversal prevention: Returns 403 for attempts like '../secrets' or paths escaping project root",
              "Glob pattern support: Accepts patterns like '**/*.py' for filtering",
              "Max files truncation: Returns truncated: true when results exceed max_files limit",
              "Router location: Implemented in app/routers/repo_view.py",
              "Service location: Logic in app/services/repo_reader.py",
              "Example calls: GET /repo/files?root=app (lists all files in app/), GET /repo/files?root=app&glob=**/*.py (lists only Python files), GET /repo/files?root=templates (lists template files)"
            ],
            "story_points": 3,
            "phase": "Phase 1",
            "epic_feature": "Repo View",
            "dependencies": ["ARCH-100"],
            "technical_notes": "Implementation locations: app/routers/repo_view.py (FastAPI router with GET /repo/files endpoint), app/services/repo_reader.py (RepoReaderService class with list_files method), app/schemas/repo_view.py (RepoFilesRequest and RepoFilesResponse Pydantic models). Allow-list: ALLOWED_ROOTS = {'app', 'templates', 'tests', 'static', 'pyproject.toml', 'README.md'} (relative to project root experience/). Forbidden paths: FORBIDDEN_NAMES = {'.env', '.git', '__pycache__', 'secrets', '.venv', 'venv', 'node_modules'} (case-insensitive check). Binary exclusion: BINARY_EXTENSIONS = {'.pyc', '.pyo', '.db', '.sqlite', '.sqlite3', '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.pdf', '.zip', '.tar', '.gz', '.exe', '.dll', '.so', '.dylib'}. Project root derivation: self.project_root = Path(__file__).resolve().parents[2] (from app/services/repo_reader.py → experience/). File traversal: Use pathlib.Path.glob(pattern) for safe file listing. Path validation: root_path.resolve().relative_to(self.project_root.resolve()) to prevent traversal. Router integration: from app.routers import repo_view; app.include_router(repo_view.router, prefix='/repo', tags=['repo'])."
          }
        },
        {
          "id": "story-repo-101",
          "type": "story",
          "data": {
            "id": "REPO-101",
            "title": "Get repo file content (read-only)",
            "user_story": "As an AI Dev Orchestrator, I want to read the content of a specific file so that I can understand existing implementations before suggesting changes.",
            "acceptance_criteria": [
              "Endpoint GET /repo/file accepts query param: path (required)",
              "Path validation: Enforces same allow-list as REPO-100 (app, templates, tests, static, pyproject.toml, README.md)",
              "Forbidden path blocking: Returns 403 for paths containing .env, .git, __pycache__, secrets, or outside allow-list",
              "Path traversal prevention: Returns 403 for attempts like '../secrets/api_key.txt'",
              "Binary file rejection: Returns 400 with error message for binary files (.pyc, .db, .sqlite, images)",
              "UTF-8 content: Returns file content as UTF-8 string",
              "Max bytes support: Accepts max_bytes param (default 16384), truncates if exceeded",
              "Response format: { path: 'app/main.py', content: 'from fastapi import...', truncated: false }",
              "Truncation flag: Sets truncated: true when content exceeds max_bytes",
              "Example calls: GET /repo/file?path=app/main.py (returns FastAPI app code), GET /repo/file?path=pyproject.toml (returns project config), GET /repo/file?path=app/routers/workspace.py (returns router code)"
            ],
            "story_points": 3,
            "phase": "Phase 1",
            "epic_feature": "Repo View",
            "dependencies": ["REPO-100"],
            "technical_notes": "Implementation locations: app/routers/repo_view.py (add GET /repo/file endpoint to existing router), app/services/repo_reader.py (extend RepoReaderService with get_file_content method). Path validation: Reuse same allow-list and forbidden path logic from REPO-100. File reading: Use Path(self.project_root / path).read_text(encoding='utf-8', errors='strict'). Binary detection: Check file extension against BINARY_EXTENSIONS, return 400 if binary. Max bytes: content = file.read_text()[:max_bytes]; truncated = len(file.read_text()) > max_bytes. UTF-8 errors: Catch UnicodeDecodeError, return 400 with message 'File is not UTF-8 encoded'. Response schema: app/schemas/repo_view.py (FileContentResponse Pydantic model)."
          }
        }
      ]
    },
    "phase_2_stories": {
      "blocks": [
        {
          "id": "phase2-overview",
          "type": "paragraph",
          "content": "Phase 2 adds workflow intelligence features that detect drift, enable version control, and improve usability. These features build on the Phase 1 foundation and prepare for multi-user collaboration in Phase 3."
        },
        {
          "id": "story-auth-200",
          "type": "story",
          "data": {
            "id": "AUTH-200",
            "title": "Migrate to third-party authentication (Auth0/Clerk)",
            "user_story": "As a user, I want to authenticate using a managed provider so that I have social login options and enhanced security.",
            "acceptance_criteria": [
              "User can authenticate via Auth0 or Clerk (OAuth providers: Google, GitHub, Email)",
              "System migrates existing magic-link users to new provider (preserve workspace ownership)",
              "Sessions table replaced with JWT token validation via provider SDK",
              "FastAPI dependency updated to validate JWT tokens from Auth0/Clerk",
              "Backward compatibility: Magic links continue working for 30 days during migration period",
              "User profile includes: email, name, avatar (from OAuth provider)",
              "Admin can disable magic-link auth after migration complete"
            ],
            "story_points": 13,
            "phase": "Phase 2",
            "epic_feature": "Authentication",
            "dependencies": ["AUTH-101"],
            "technical_notes": "Implementation locations: app/routers/auth.py (add OAuth callbacks), app/services/auth_service.py (AuthService with provider SDK integration), app/models/users.py (new User model with provider_id field). Migration: CREATE TABLE users (id UUID, email TEXT, provider TEXT, provider_id TEXT, created_at TEXT). Map existing sessions.email to users.email. Auth0/Clerk SDK: Initialize client with environment variables (AUTH0_DOMAIN, AUTH0_CLIENT_ID, AUTH0_CLIENT_SECRET or CLERK_SECRET_KEY). OAuth flow: Redirect to provider login → callback endpoint validates code → create session/JWT → redirect to app. Backward compatibility: Keep sessions table active, check both sessions and JWT in get_current_user dependency. Disable magic-link: Add feature flag in environment variables."
          }
        },
        {
          "id": "story-drift-100",
          "type": "story",
          "data": {
            "id": "DRIFT-100",
            "title": "Drift detection for manual edits",
            "user_story": "As a user, I want to be warned when my manual edits conflict with the AI-generated canonical documents so that I can reconcile changes intentionally.",
            "acceptance_criteria": [
              "System detects when user manually edits approved canonical document (Epic, Architecture, Backlog)",
              "System compares current approved version with latest superseded version",
              "If differences exceed threshold (e.g., >20% sections changed), show drift warning",
              "Warning displays: 'Your edits differ significantly from AI-generated version. [View Diff] [Accept Edits] [Revert]'",
              "View Diff: Shows side-by-side comparison of AI version vs. edited version",
              "Accept Edits: Marks document as 'user_modified' status, disables auto-regeneration warnings",
              "Revert: Restores latest AI-generated version (marks current as superseded)",
              "Drift detection runs on document save (inline edit PATCH endpoints)"
            ],
            "story_points": 8,
            "phase": "Phase 2",
            "epic_feature": "Workflow Intelligence",
            "dependencies": ["EPIC-100", "ARCH-101", "BACKLOG-101"],
            "technical_notes": "Implementation locations: app/services/drift_detector.py (DriftDetector.detect_drift method), app/routers/epic.py, app/routers/architecture.py, app/routers/backlog.py (call drift detector on PATCH). Diff algorithm: Use difflib.SequenceMatcher to compare JSON content. Threshold: ratio = SequenceMatcher(None, original, edited).ratio(); if ratio < 0.8: show_drift_warning(). Status field: Add 'user_modified' to canonical_documents.status enum. Diff view: templates/components/diff_view.html (side-by-side comparison with highlight). Accept/Revert: PATCH endpoints update status or restore superseded version."
          }
        },
        {
          "id": "story-version-100",
          "type": "story",
          "data": {
            "id": "VERSION-100",
            "title": "Version history with rollback",
            "user_story": "As a user, I want to view all previous versions of my Epic, Architecture, and Backlog so that I can rollback to an earlier version if needed.",
            "acceptance_criteria": [
              "User clicks 'View History' button on Epic/Architecture/Backlog approval page",
              "System shows list of all versions sorted by created_at DESC",
              "Each version shows: created_at timestamp, status (approved/superseded/user_modified), preview (first 200 chars)",
              "User can expand version to view full content",
              "User can compare two versions side-by-side (select 2 versions → [Compare])",
              "User can rollback to previous version: [Restore This Version]",
              "Restore: Creates new draft with content from selected version, marks current as superseded",
              "Version history preserved indefinitely (no automatic deletion)"
            ],
            "story_points": 5,
            "phase": "Phase 2",
            "epic_feature": "Workflow Intelligence",
            "dependencies": ["EPIC-100", "ARCH-101", "BACKLOG-101"],
            "technical_notes": "Implementation locations: app/routers/epic.py, app/routers/architecture.py, app/routers/backlog.py (GET /workspaces/{id}/epic/versions, GET /workspaces/{id}/architecture/versions, GET /workspaces/{id}/backlog/versions endpoints), templates/components/version_history.html. Query: SELECT id, created_at, status, content FROM canonical_documents WHERE workspace_id=... AND doc_type='epic' ORDER BY created_at DESC. Compare: Use difflib for side-by-side comparison (same as DRIFT-100). Restore: INSERT new row with status='draft', content from selected version; UPDATE current version SET status='superseded'. Template: Accordion UI with expand/collapse for each version."
          }
        },
        {
          "id": "story-library-100",
          "type": "story",
          "data": {
            "id": "LIBRARY-100",
            "title": "Idea library with search",
            "user_story": "As a user, I want to search across all my workspaces so that I can quickly find past ideas and reuse components.",
            "acceptance_criteria": [
              "User enters search query in global search bar (available in navigation)",
              "System searches workspace names, descriptions, and canonical document content (Epic, Architecture, Backlog)",
              "Results grouped by workspace, showing: workspace name, matched snippet (with query highlighted), last updated timestamp",
              "User can filter results by: workspace status (active/archived), doc_type (epic/architecture/backlog), date range",
              "Clicking result navigates to workspace detail or specific document section",
              "Search supports wildcards (*, ?) and boolean operators (AND, OR, NOT)",
              "Results ranked by relevance (TF-IDF or full-text search scoring)"
            ],
            "story_points": 8,
            "phase": "Phase 2",
            "epic_feature": "Workflow Intelligence",
            "dependencies": ["WRK-100", "EPIC-100"],
            "technical_notes": "Implementation locations: app/routers/search.py (GET /search endpoint), app/services/search_service.py (SearchService with full-text search logic), templates/search/results.html. SQLite full-text search: CREATE VIRTUAL TABLE workspace_search USING fts5(workspace_id, content); populate from workspaces.name, workspaces.description, canonical_documents.content. Query: SELECT workspace_id, snippet(workspace_search, 1, '<mark>', '</mark>', '...', 32) FROM workspace_search WHERE content MATCH ?. Ranking: Use FTS5 rank. Filters: WHERE clauses for status, doc_type, created_at. Highlight: Use FTS5 snippet function. Navigation: Include section_id in result links for deep linking to specific Epic/Architecture/Backlog sections."
          }
        },
        {
          "id": "story-pdf-100",
          "type": "story",
          "data": {
            "id": "PDF-100",
            "title": "PDF export generation",
            "user_story": "As a user, I want to export my product definition as a PDF so that I can share professionally formatted documentation with stakeholders.",
            "acceptance_criteria": [
              "User clicks 'Export as PDF' button on workspace detail page",
              "System generates PDF with cover page (workspace name, logo, date), table of contents, Epic sections, Architecture sections (including diagrams), Backlog sections (stories in table format)",
              "PDF includes page numbers, headers (workspace name), footers (generated date)",
              "Diagrams rendered as images (convert ASCII diagrams to PNG/SVG)",
              "Code blocks syntax-highlighted",
              "PDF saved to experience/tmp/exports/{workspace_id}_{timestamp}.pdf",
              "System returns FileResponse with Content-Disposition: attachment; filename='{workspace_name}.pdf'",
              "Temporary file deleted after 1 hour"
            ],
            "story_points": 8,
            "phase": "Phase 2",
            "epic_feature": "Export",
            "dependencies": ["EXPORT-100"],
            "technical_notes": "Implementation locations: app/routers/export.py (GET /workspaces/{id}/export?format=pdf endpoint), app/services/export_service.py (ExportService.generate_pdf method using ReportLab or WeasyPrint). PDF library: Use WeasyPrint (HTML → PDF, better CSS support) or ReportLab (programmatic PDF generation). Template: templates/export/pdf_export.html (HTML template with print-friendly CSS). Cover page: Include workspace.name, generated timestamp, optional logo. Table of contents: Auto-generate from section headings with page numbers. Diagrams: Convert ASCII art to images using Pillow or matplotlib. Code highlighting: Use Pygments for syntax highlighting in HTML, then render to PDF. FileResponse: media_type='application/pdf'."
          }
        },
        {
          "id": "story-migrate-100",
          "type": "story",
          "data": {
            "id": "MIGRATE-100",
            "title": "PostgreSQL migration",
            "user_story": "As a platform administrator, I want to migrate from SQLite to PostgreSQL so that the system can scale beyond hundreds of concurrent users.",
            "acceptance_criteria": [
              "System detects PostgreSQL connection string in environment variable DATABASE_URL",
              "If PostgreSQL configured, system uses PostgreSQL; otherwise fallback to SQLite",
              "Database schema identical between SQLite and PostgreSQL (via SQLAlchemy)",
              "Migration script: python -m alembic upgrade head applies schema to PostgreSQL",
              "Data migration: python scripts/migrate_sqlite_to_postgres.py copies all data from SQLite to PostgreSQL",
              "JSON columns use PostgreSQL JSONB (with GIN indexes for performance)",
              "No application code changes required (SQLAlchemy abstracts database differences)",
              "Rollback plan: Keep SQLite backup, script to reverse migration if needed"
            ],
            "story_points": 13,
            "phase": "Phase 2",
            "epic_feature": "Infrastructure",
            "dependencies": [],
            "technical_notes": "Implementation locations: app/main.py (database connection logic reads DATABASE_URL), alembic/ (migration scripts), scripts/migrate_sqlite_to_postgres.py (data migration script). SQLAlchemy engine: if os.getenv('DATABASE_URL'): engine = create_engine(os.getenv('DATABASE_URL')) else: engine = create_engine('sqlite:///./workbench.db'). Alembic: alembic init alembic; configure alembic.ini with sqlalchemy.url. Migration: alembic revision --autogenerate -m 'Initial schema'; alembic upgrade head. Data migration: Read from SQLite, write to PostgreSQL in batches. JSONB: Use sqlalchemy.dialects.postgresql.JSONB for canonical_documents.content. GIN index: CREATE INDEX idx_canonical_documents_content ON canonical_documents USING GIN (content). Testing: Run full test suite against PostgreSQL before production migration."
          }
        },
        {
          "id": "story-repo-200",
          "type": "story",
          "data": {
            "id": "REPO-200",
            "title": "Search repo by content (read-only)",
            "user_story": "As an AI Dev Orchestrator, I want to search for specific text across the repository so that I can find implementations or usages of specific functions, classes, or patterns.",
            "acceptance_criteria": [
              "Endpoint GET /repo/search accepts query params: q (required), glob (optional), max_results (optional, default 50)",
              "Search scope: Searches only within allow-listed roots (app, templates, tests, static, pyproject.toml, README.md)",
              "Case-insensitive search: Matches query string regardless of case",
              "Result format: Returns array of { path: 'app/services/repo_reader.py', line_number: 42, snippet: '...' }",
              "Snippet context: Shows matched line plus ±1 line of surrounding context",
              "Glob pattern support: Accepts patterns like '**/*.py' to narrow search scope",
              "Max results: Limits results to max_results parameter",
              "Forbidden path exclusion: Skips .env, .git, __pycache__, secrets during search",
              "Binary file exclusion: Skips binary files (.pyc, .db, .sqlite, images)",
              "Example searches: q=FastAPI (finds FastAPI imports in app/main.py, app/routers/*), q=canonical_documents (finds database table references across app/), q=workspace_id (finds foreign key usage in models and services)"
            ],
            "story_points": 5,
            "phase": "Phase 2",
            "epic_feature": "Repo View",
            "dependencies": ["REPO-100", "REPO-101"],
            "technical_notes": "Implementation locations: app/routers/repo_view.py (GET /repo/search endpoint), app/services/repo_reader.py (RepoReaderService.search_content method). Search algorithm: Iterate files from REPO-100 allow-list, read content via REPO-101 logic, perform case-insensitive search using re.search(query, content, re.IGNORECASE). Context extraction: When match found at line N, extract lines N-1, N, N+1 as snippet. Response: List of {path, line_number, snippet}. Glob filter: Apply glob pattern before searching to reduce file count. Max results: Break loop when len(results) >= max_results. Binary/forbidden exclusion: Reuse REPO-100 filters."
          }
        }
      ]
    },
    "phase_3_stories": {
      "blocks": [
        {
          "id": "phase3-overview",
          "type": "paragraph",
          "content": "Phase 3 enables growth and expansion with multi-user collaboration, API access for developers, and user-selectable LLM models. These features position the platform for scaling beyond solo founders to small teams and power users."
        },
        {
          "id": "story-share-100",
          "type": "story",
          "data": {
            "id": "SHARE-100",
            "title": "Workspace sharing (read-only)",
            "user_story": "As a user, I want to share my workspace with teammates so that they can view my product definition without editing it.",
            "acceptance_criteria": [
              "User clicks 'Share' button on workspace detail page",
              "System shows modal: 'Share workspace - Enter email addresses (comma-separated)'",
              "User enters email addresses, clicks 'Share as Read-Only'",
              "System creates workspace_shares records (workspace_id, email, permission='read')",
              "System sends email notification to each recipient with link to workspace",
              "Recipient can view workspace, Epic, Architecture, Backlog (all pages read-only)",
              "Recipient cannot edit, approve, regenerate, or delete workspace",
              "Owner can revoke access: 'Remove' button next to each shared user",
              "Owner can upgrade to 'edit' permission (future story)"
            ],
            "story_points": 8,
            "phase": "Phase 3",
            "epic_feature": "Collaboration",
            "dependencies": ["AUTH-200", "WRK-100"],
            "technical_notes": "Implementation locations: app/routers/workspace.py (POST /workspaces/{id}/share, DELETE /workspaces/{id}/share/{email} endpoints), app/models/workspace_shares.py (WorkspaceShare SQLAlchemy model), templates/workspace/share_modal.html. Schema: CREATE TABLE workspace_shares (id UUID, workspace_id UUID REFERENCES workspaces(id), email TEXT, permission TEXT DEFAULT 'read', created_at TEXT). Authorization: Update get_current_user dependency to check: workspace.user_id == current_user.id OR workspace_shares.email == current_user.email. Read-only enforcement: Disable edit buttons in templates via Jinja2: {% if user_is_owner %}[Edit button]{% endif %}. Email notification: Use email_service.py from AUTH-101. Revoke: DELETE FROM workspace_shares WHERE workspace_id=... AND email=...."
          }
        },
        {
          "id": "story-api-100",
          "type": "story",
          "data": {
            "id": "API-100",
            "title": "API key authentication for programmatic access",
            "user_story": "As a developer, I want to generate API keys so that I can access Workbench AI programmatically via REST API.",
            "acceptance_criteria": [
              "User navigates to 'Settings → API Keys'",
              "User clicks 'Generate New API Key', system creates api_keys record with key (UUID), user_id, name (optional), created_at",
              "System displays API key once (never shown again), user copies key",
              "User makes API requests with header: Authorization: Bearer <api_key>",
              "FastAPI dependency validates API key from header, returns user context",
              "API keys have same permissions as user (can CRUD their own workspaces)",
              "User can list all their API keys (shows name, created_at, last_used_at)",
              "User can revoke API key (DELETE /api-keys/{id}), immediately invalidates key",
              "Rate limiting: API keys subject to same 60 requests/minute limit as web users"
            ],
            "story_points": 8,
            "phase": "Phase 3",
            "epic_feature": "Developer Access",
            "dependencies": ["AUTH-200"],
            "technical_notes": "Implementation locations: app/routers/api_keys.py (POST /api-keys, GET /api-keys, DELETE /api-keys/{id} endpoints), app/models/api_keys.py (APIKey SQLAlchemy model), app/dependencies.py (get_current_user_from_api_key dependency). Schema: CREATE TABLE api_keys (id UUID, user_id UUID REFERENCES users(id), key_hash TEXT, name TEXT, created_at TEXT, last_used_at TEXT). Key generation: Use secrets.token_urlsafe(32) for key, store bcrypt hash. Authentication: async def get_current_user_from_api_key(authorization: str = Header(None)): if not authorization or not authorization.startswith('Bearer '): raise HTTPException(401); key = authorization.split(' ')[1]; api_key = db.query(APIKey).filter(APIKey.key_hash == bcrypt.hashpw(key, salt)).first(); if not api_key: raise HTTPException(401); return api_key.user. Template: templates/settings/api_keys.html."
          }
        },
        {
          "id": "story-model-100",
          "type": "story",
          "data": {
            "id": "MODEL-100",
            "title": "User-selectable LLM models",
            "user_story": "As a power user, I want to choose which Claude model to use for each agent role (PM, Architect, BA) so that I can optimize for quality vs. cost.",
            "acceptance_criteria": [
              "User navigates to 'Settings → Agent Models'",
              "User sees dropdown for each agent role: PM Agent, Architect Agent, BA Agent, with options: Claude Opus 4 (high quality, high cost), Claude Sonnet 4.5 (balanced, default), Claude Haiku 4.5 (fast, low cost)",
              "User selects model per role, clicks 'Save Settings'",
              "System stores preferences in workspaces.model_config (JSON field: {pm_model: 'claude-opus-4-20250514', architect_model: 'claude-sonnet-4-5-20250929', ba_model: 'claude-haiku-4-5-20251001'})",
              "Agent services read model_config from workspace, use specified model for API calls",
              "If no config set, fallback to default: claude-sonnet-4-5-20250929 for all roles",
              "Settings page shows estimated cost per workflow based on model selection",
              "User can reset to defaults"
            ],
            "story_points": 5,
            "phase": "Phase 3",
            "epic_feature": "Customization",
            "dependencies": ["WRK-100"],
            "technical_notes": "Implementation locations: app/routers/settings.py (GET /settings/models, POST /settings/models endpoints), app/services/pm_agent_service.py, app/services/architecture_agent_service.py, app/services/backlog_agent_service.py (read model from workspace.model_config). Schema: ALTER TABLE workspaces ADD COLUMN model_config JSON. Default config: {pm_model: 'claude-sonnet-4-5-20250929', architect_model: 'claude-sonnet-4-5-20250929', ba_model: 'claude-sonnet-4-5-20250929'}. Model selection: config = workspace.model_config or default_config; model = config.get('pm_model'); response = await anthropic_client.messages.create(model=model, ...). Cost estimation: Hardcode token costs per model, multiply by estimated tokens per workflow. Template: templates/settings/models.html with dropdowns."
          }
        }
      ]
    },
    "backlog_summary": {
      "blocks": [
        {
          "id": "summary-table",
          "type": "table",
          "data": {
            "headers": ["Phase", "Stories", "Story Points", "Duration", "Key Features"],
            "rows": [
              ["Phase 1 (MVP)", "22", "117", "3 months", "Magic-link auth, Workspace CRUD, PM Discovery, Epic/Architecture/Backlog generation, Markdown/JSON export, SSE streaming, Error handling, Pipeline re-runs, Repo introspection (read-only)"],
              ["Phase 2 (Intelligence)", "7", "52", "2-3 months", "Third-party auth migration, Drift detection, Version history, Idea library search, PDF export, PostgreSQL migration, Repo content search"],
              ["Phase 3 (Growth)", "3", "29", "2-3 months", "Workspace sharing (read-only), API key authentication, User-selectable LLM models"],
              ["Total", "32", "198", "7-9 months", "Complete product definition platform with AI orchestration, canonical document storage, and developer tools"]
            ]
          }
        },
        {
          "id": "velocity-estimate",
          "type": "paragraph",
          "content": "Estimated team velocity: 39 story points per month with 2-3 engineers. Phase 1 MVP (117 points) = 3 months. Phase 2 (52 points) = 1.5-2 months. Phase 3 (29 points) = 1 month. Total timeline: 7-9 months including buffer for unknowns, holidays, and ramp-up time."
        },
        {
          "id": "dependencies-note",
          "type": "paragraph",
          "content": "Critical path: AUTH-101 → WRK-100 → PM-100 → PM-101 → EPIC-100 → ARCH-100 → ARCH-101 → BACKLOG-100 → BACKLOG-101 → EXPORT-100. Most other stories can be developed in parallel after their dependencies are met. REPO-100 and REPO-101 are independent and can be built early. SSE-100 should be integrated throughout PM/Architect/BA stages."
        },
        {
          "id": "architectural-alignment",
          "type": "paragraph",
          "content": "All stories reconciled with Canonical Architecture V1.1. Repository structure: experience/ (project root), app/ (Python package with main.py, routers/, services/, schemas/, models/). Import pattern: from app.<submodule> import <object>. Execution model: cd experience; uvicorn app.main:app --reload. All file paths, implementation locations, and technical notes updated to reflect canonical structure. No deprecated patterns (experience/main.py, from experience.app.* imports) present in backlog."
        }
      ]
    }
  }
}